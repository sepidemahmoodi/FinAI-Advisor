{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 4: Advanced ML - Ensembles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "LOADING DATA AND FEATURE ENGINEERING\n",
            "======================================================================\n",
            "Loaded 10000 expense records\n",
            "Columns: ['date', 'description', 'amount', 'category']\n",
            "\n",
            "First few rows:\n",
            "         date                                description      amount  \\\n",
            "0  2024-09-04           Life order relate idea a follow.  286.896893   \n",
            "1  2025-01-24        Themselves middle break yet notice.  192.898054   \n",
            "2  2024-01-31  Account region available natural perhaps.  224.936664   \n",
            "3  2025-12-07                    Office conference drop.  117.974528   \n",
            "4  2025-03-09         Yet money many moment choice door.   88.962677   \n",
            "\n",
            "        category  \n",
            "0     Dining Out  \n",
            "1     Healthcare  \n",
            "2      Utilities  \n",
            "3  Entertainment  \n",
            "4     Healthcare  \n",
            "\n",
            "‚ö†Ô∏è  No family data columns found (sibsp, parch)\n",
            "\n",
            "======================================================================\n",
            "ADDING LAG FEATURES (Time-Series Features from Week 2)\n",
            "======================================================================\n",
            "‚úÖ Created lag features:\n",
            "   - amount_lag1: Previous period's spending\n",
            "   - amount_lag2: Two periods ago spending\n",
            "   - amount_rolling_mean_3: 3-period moving average\n",
            "\n",
            "Example of lag features:\n",
            "        date      amount  amount_lag1  amount_lag2  amount_rolling_mean_3\n",
            "0 2024-01-01   24.936609          NaN          NaN              24.936609\n",
            "1 2024-01-01  443.133074    24.936609          NaN             234.034841\n",
            "2 2024-01-01  196.019094   443.133074    24.936609             221.362925\n",
            "3 2024-01-01   92.143520   196.019094   443.133074             243.765229\n",
            "4 2024-01-01  344.713315    92.143520   196.019094             210.958643\n",
            "5 2024-01-01  389.096442   344.713315    92.143520             275.317759\n",
            "6 2024-01-01  208.439203   389.096442   344.713315             314.082987\n",
            "7 2024-01-02  397.892870   208.439203   389.096442             331.809505\n",
            "8 2024-01-02  458.770100   397.892870   208.439203             355.034058\n",
            "9 2024-01-02  156.214408   458.770100   397.892870             337.625793\n",
            "\n",
            "======================================================================\n",
            "ADDING CLUSTER FEATURES (User Segments from Week 3)\n",
            "======================================================================\n",
            "‚úÖ Created cluster feature:\n",
            "   - spending_cluster: User spending segment (Low/Medium/High)\n",
            "   - cluster_id: Numeric cluster ID (0/1/2)\n",
            "\n",
            "Cluster distribution:\n",
            "spending_cluster\n",
            "Medium    4747\n",
            "High      3298\n",
            "Low       1955\n",
            "Name: count, dtype: int64\n",
            "\n",
            "======================================================================\n",
            "PREPARING FEATURES AND TARGET\n",
            "======================================================================\n",
            "‚úÖ Target: 'category' (Classification - predicting expense category)\n",
            "\n",
            "‚úÖ Features (X) shape: (10000, 7)\n",
            "   - 10000 samples (rows)\n",
            "   - 7 features (columns)\n",
            "\n",
            "‚úÖ Target (y) shape: (10000,)\n",
            "\n",
            "Feature columns: ['date', 'description', 'amount', 'amount_lag1', 'amount_lag2', 'amount_rolling_mean_3', 'cluster_id']\n",
            "\n",
            "======================================================================\n",
            "SUMMARY: KEY CONCEPTS EXPLAINED\n",
            "======================================================================\n",
            "1. FEATURE ENGINEERING: Creating new features from existing data\n",
            "2. LAG FEATURES: Using past values to predict future (time-series)\n",
            "3. CLUSTER FEATURES: Using user segments as features\n",
            "4. FEATURE PREPARATION: Organizing data for machine learning\n",
            "======================================================================\n",
            "\n",
            "üìä Final dataset preview:\n",
            "        date                             description      amount    category  \\\n",
            "0 2024-01-01  No necessary successful medical thing.   24.936609   Groceries   \n",
            "1 2024-01-01                    Reduce full meeting.  443.133074   Transport   \n",
            "2 2024-01-01  Certain resource power serve research.  196.019094   Utilities   \n",
            "3 2024-01-01    Admit international give early race.   92.143520   Groceries   \n",
            "4 2024-01-01         Performance party media matter.  344.713315  Dining Out   \n",
            "\n",
            "   amount_lag1  amount_lag2  amount_rolling_mean_3 spending_cluster cluster_id  \n",
            "0          NaN          NaN              24.936609              Low          0  \n",
            "1    24.936609          NaN             234.034841             High          2  \n",
            "2   443.133074    24.936609             221.362925           Medium          1  \n",
            "3   196.019094   443.133074             243.765229              Low          0  \n",
            "4    92.143520   196.019094             210.958643             High          2  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ============================================================================\n",
        "# WEEK 4: ADVANCED ML - ENSEMBLE METHODS\n",
        "# ============================================================================\n",
        "# This notebook combines concepts from previous weeks:\n",
        "# - Week 1: Classification (expense categorization)\n",
        "# - Week 2: Time-series features (lagging features for forecasting)\n",
        "# - Week 3: Clustering (user segments)\n",
        "# - Week 4: Ensemble methods (combining multiple models)\n",
        "#\n",
        "# ENSEMBLE LEARNING: Using multiple models together to make better predictions\n",
        "# - Idea: \"Wisdom of the crowd\" - multiple opinions are better than one\n",
        "# - Types: Bagging, Boosting, Stacking\n",
        "# - Benefits: Better accuracy, more robust, handles overfitting\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"LOADING DATA AND FEATURE ENGINEERING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load data from previous weeks\n",
        "df = pd.read_csv('../data/synthetic_expenses.csv')\n",
        "\n",
        "print(f\"Loaded {len(df)} expense records\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# ============================================================================\n",
        "# CONCEPT 1: FEATURE ENGINEERING - Creating New Features\n",
        "# ============================================================================\n",
        "# Feature Engineering: Creating new features from existing data to improve\n",
        "# model performance. Good features = better predictions!\n",
        "#\n",
        "# Types of Feature Engineering:\n",
        "# 1. Derived Features: Combine existing features (e.g., family_size)\n",
        "# 2. Lag Features: Use past values (from Week 2 - time-series)\n",
        "# 3. Cluster Features: Use cluster assignments (from Week 3 - segmentation)\n",
        "# 4. Aggregated Features: Summarize groups (e.g., average spending)\n",
        "# ============================================================================\n",
        "\n",
        "# Example: Derived feature (if we had family data)\n",
        "# This is just an example - your data might not have these columns\n",
        "if 'sibsp' in df.columns and 'parch' in df.columns:\n",
        "    df['family_size'] = df['sibsp'] + df['parch'] + 1\n",
        "    print(\"\\n‚úÖ Created derived feature: family_size\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  No family data columns found (sibsp, parch)\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONCEPT 2: LAG FEATURES (From Week 2 - Time Series)\n",
        "# ============================================================================\n",
        "# Lag Features: Using PAST values to predict FUTURE values\n",
        "#\n",
        "# What is \"Lag\"?\n",
        "# - Lag-1: Value from 1 time period ago (previous month)\n",
        "# - Lag-2: Value from 2 time periods ago (2 months ago)\n",
        "# - Lag-n: Value from n time periods ago\n",
        "#\n",
        "# Why Use Lag Features?\n",
        "# - Time-series data has temporal dependencies\n",
        "# - Past spending predicts future spending\n",
        "# - Example: If you spent $1000 last month, you might spend similarly this month\n",
        "#\n",
        "# How It Works:\n",
        "#   Original:  [100, 150, 200, 180, 220]\n",
        "#   Lag-1:     [NaN, 100, 150, 200, 180]  ‚Üê Shifted by 1\n",
        "#   Lag-2:     [NaN, NaN, 100, 150, 200]  ‚Üê Shifted by 2\n",
        "#\n",
        "# Real Example:\n",
        "#   Month 1: Spend $1000 ‚Üí Lag-1 for Month 2 = $1000\n",
        "#   Month 2: Spend $1200 ‚Üí Lag-1 for Month 3 = $1200\n",
        "#   Month 3: Spend $1100 ‚Üí Lag-1 for Month 4 = $1100\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ADDING LAG FEATURES (Time-Series Features from Week 2)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check if we have date column for time-series features\n",
        "if 'date' in df.columns:\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df = df.sort_values('date').reset_index(drop=True)\n",
        "    \n",
        "    # Create lag features for amount (spending patterns)\n",
        "    # Lag-1: Previous period's spending\n",
        "    df['amount_lag1'] = df['amount'].shift(1)\n",
        "    \n",
        "    # Lag-2: Two periods ago spending\n",
        "    df['amount_lag2'] = df['amount'].shift(2)\n",
        "    \n",
        "    # Rolling average (moving average) - another time-series feature\n",
        "    df['amount_rolling_mean_3'] = df['amount'].rolling(window=3, min_periods=1).mean()\n",
        "    \n",
        "    print(\"‚úÖ Created lag features:\")\n",
        "    print(\"   - amount_lag1: Previous period's spending\")\n",
        "    print(\"   - amount_lag2: Two periods ago spending\")\n",
        "    print(\"   - amount_rolling_mean_3: 3-period moving average\")\n",
        "    print(\"\\nExample of lag features:\")\n",
        "    print(df[['date', 'amount', 'amount_lag1', 'amount_lag2', 'amount_rolling_mean_3']].head(10))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No 'date' column found - skipping lag features\")\n",
        "    print(\"   (Lag features require time-series data)\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONCEPT 3: CLUSTER FEATURES (From Week 3 - User Segmentation)\n",
        "# ============================================================================\n",
        "# Cluster Features: Using cluster assignments as features\n",
        "#\n",
        "# What is a Cluster?\n",
        "# - A group of similar data points\n",
        "# - In Week 3, we grouped users by spending behavior\n",
        "# - Example clusters: \"Big Spenders\", \"Budget Conscious\", \"Entertainment Lovers\"\n",
        "#\n",
        "# Why Use Cluster Features?\n",
        "# - Captures complex patterns in one categorical feature\n",
        "# - Users in same cluster have similar behavior\n",
        "# - Helps model understand user segments\n",
        "#\n",
        "# How It Works:\n",
        "#   User A: Spending pattern ‚Üí Cluster 0 (\"Big Spender\")\n",
        "#   User B: Spending pattern ‚Üí Cluster 0 (\"Big Spender\")\n",
        "#   User C: Spending pattern ‚Üí Cluster 1 (\"Budget Conscious\")\n",
        "#\n",
        "#   Cluster becomes a feature: cluster = 0 or 1 or 2...\n",
        "#\n",
        "# Real Example:\n",
        "#   - Cluster 0 users: High spending across all categories\n",
        "#   - Cluster 1 users: Low spending, focus on essentials\n",
        "#   - Cluster 2 users: High entertainment, low utilities\n",
        "#\n",
        "# The model learns: \"Users in Cluster 0 tend to spend more\"\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ADDING CLUSTER FEATURES (User Segments from Week 3)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# In a real scenario, you would load cluster assignments from Week 3\n",
        "# For now, we'll create a simple example cluster feature\n",
        "# (In practice, this would come from your Week 3 clustering results)\n",
        "\n",
        "# Example: Create cluster based on spending amount (simplified)\n",
        "if 'amount' in df.columns:\n",
        "    # Simple clustering: High/Medium/Low spenders\n",
        "    df['spending_cluster'] = pd.cut(\n",
        "        df['amount'],\n",
        "        bins=[0, 100, 300, float('inf')],\n",
        "        labels=['Low', 'Medium', 'High']\n",
        "    )\n",
        "    \n",
        "    # Convert to numeric for model (if needed)\n",
        "    cluster_map = {'Low': 0, 'Medium': 1, 'High': 2}\n",
        "    df['cluster_id'] = df['spending_cluster'].map(cluster_map)\n",
        "    \n",
        "    print(\"‚úÖ Created cluster feature:\")\n",
        "    print(\"   - spending_cluster: User spending segment (Low/Medium/High)\")\n",
        "    print(\"   - cluster_id: Numeric cluster ID (0/1/2)\")\n",
        "    print(\"\\nCluster distribution:\")\n",
        "    print(df['spending_cluster'].value_counts())\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No 'amount' column found - skipping cluster features\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONCEPT 4: PREPARING DATA FOR ENSEMBLE MODELS\n",
        "# ============================================================================\n",
        "# Now we prepare features (X) and target (y) for machine learning\n",
        "#\n",
        "# Features (X): What the model uses to make predictions\n",
        "#   - Original features: amount, description, date\n",
        "#   - Engineered features: lag features, cluster features\n",
        "#   - All features together help the model learn patterns\n",
        "#\n",
        "# Target (y): What we want to predict\n",
        "#   - Classification: category (which expense category?)\n",
        "#   - Regression: amount (how much will be spent?)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PREPARING FEATURES AND TARGET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Prepare features (X) - everything except the target\n",
        "# Drop target column and any non-feature columns\n",
        "columns_to_drop = []\n",
        "if 'category' in df.columns:\n",
        "    columns_to_drop.append('category')\n",
        "if 'spending_cluster' in df.columns:\n",
        "    columns_to_drop.append('spending_cluster')  # Keep cluster_id, drop label\n",
        "\n",
        "X = df.drop(columns_to_drop, axis=1, errors='ignore')\n",
        "\n",
        "# Prepare target (y) - what we want to predict\n",
        "if 'category' in df.columns:\n",
        "    y = df['category']  # Classification task\n",
        "    print(\"‚úÖ Target: 'category' (Classification - predicting expense category)\")\n",
        "elif 'amount' in df.columns:\n",
        "    y = df['amount']  # Regression task\n",
        "    print(\"‚úÖ Target: 'amount' (Regression - predicting spending amount)\")\n",
        "else:\n",
        "    y = None\n",
        "    print(\"‚ö†Ô∏è  No target column found\")\n",
        "\n",
        "# Handle missing values from lag features\n",
        "X = X.fillna(0)  # Replace NaN with 0 (or use forward fill, mean, etc.)\n",
        "\n",
        "print(f\"\\n‚úÖ Features (X) shape: {X.shape}\")\n",
        "print(f\"   - {X.shape[0]} samples (rows)\")\n",
        "print(f\"   - {X.shape[1]} features (columns)\")\n",
        "print(f\"\\n‚úÖ Target (y) shape: {y.shape if y is not None else 'None'}\")\n",
        "print(f\"\\nFeature columns: {list(X.columns)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUMMARY: KEY CONCEPTS EXPLAINED\")\n",
        "print(\"=\"*70)\n",
        "print(\"1. FEATURE ENGINEERING: Creating new features from existing data\")\n",
        "print(\"2. LAG FEATURES: Using past values to predict future (time-series)\")\n",
        "print(\"3. CLUSTER FEATURES: Using user segments as features\")\n",
        "print(\"4. FEATURE PREPARATION: Organizing data for machine learning\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüìä Final dataset preview:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ENSEMBLE LEARNING - COMPLETE GUIDE\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "1. WHAT IS ENSEMBLE LEARNING?\n",
            "======================================================================\n",
            "\n",
            "Ensemble Learning = Combining multiple models for better predictions\n",
            "\n",
            "Key Idea: \"Wisdom of the Crowd\"\n",
            "- Multiple models ‚Üí Multiple perspectives\n",
            "- Errors cancel out ‚Üí More robust predictions\n",
            "- Different models catch different patterns ‚Üí Better coverage\n",
            "\n",
            "Real-World Analogy:\n",
            "  Single Model: One weather forecaster\n",
            "  Ensemble: 100 weather forecasters voting\n",
            "  Result: More accurate weather prediction\n",
            "\n",
            "\n",
            "======================================================================\n",
            "2. WHY USE ENSEMBLE LEARNING?\n",
            "======================================================================\n",
            "\n",
            "Benefits:\n",
            "‚úÖ Better Accuracy: 5-20% improvement over single models\n",
            "‚úÖ More Robust: Less sensitive to outliers and noise\n",
            "‚úÖ Reduces Overfitting: Multiple models prevent memorization\n",
            "‚úÖ Handles Complexity: Captures patterns single models miss\n",
            "‚úÖ Error Reduction: Mistakes cancel out\n",
            "\n",
            "Trade-offs:\n",
            "‚ö†Ô∏è  More computational cost (train multiple models)\n",
            "‚ö†Ô∏è  More complex to understand and explain\n",
            "‚ö†Ô∏è  Takes longer to train and predict\n",
            "\n",
            "\n",
            "======================================================================\n",
            "3. TYPE 1: BAGGING (Bootstrap Aggregating)\n",
            "======================================================================\n",
            "\n",
            "Definition: Train multiple models on different random data subsets,\n",
            "            then combine their predictions\n",
            "\n",
            "How It Works:\n",
            "  1. Create bootstrap samples (random sampling WITH replacement)\n",
            "  2. Train model on each sample (same algorithm, different data)\n",
            "  3. Predict: Average (regression) or Majority Vote (classification)\n",
            "\n",
            "Key Characteristics:\n",
            "  ‚úÖ Models trained in PARALLEL (independent, can train simultaneously)\n",
            "  ‚úÖ Reduces VARIANCE (less sensitive to data changes)\n",
            "  ‚úÖ Works well with high-variance models (Decision Trees, Neural Nets)\n",
            "  ‚úÖ Each model sees ~63% of data (bootstrap sampling)\n",
            "\n",
            "Bootstrap Sampling:\n",
            "  - Random sampling WITH replacement\n",
            "  - Sample size = original dataset size\n",
            "  - Some samples appear multiple times, some not at all\n",
            "  - Each sample is different ‚Üí different models\n",
            "\n",
            "Example: Random Forest\n",
            "  - 100 Decision Trees, each on different bootstrap sample\n",
            "  - Final prediction = majority vote of all trees\n",
            "  - More trees = better (but diminishing returns)\n",
            "\n",
            "When to Use:\n",
            "  ‚úÖ High variance models (overfitting)\n",
            "  ‚úÖ Large datasets\n",
            "  ‚úÖ Want parallel training (faster)\n",
            "  ‚úÖ Need robust predictions\n",
            "\n",
            "\n",
            "======================================================================\n",
            "4. TYPE 2: BOOSTING\n",
            "======================================================================\n",
            "\n",
            "Definition: Train models SEQUENTIALLY, each learning from previous mistakes\n",
            "\n",
            "How It Works:\n",
            "  1. Train Model 1 on all data\n",
            "  2. Identify mistakes (misclassified examples)\n",
            "  3. Train Model 2 focusing on mistakes (give them more weight)\n",
            "  4. Train Model 3 focusing on Model 2's mistakes\n",
            "  5. Continue: Each model corrects previous errors\n",
            "  6. Combine with weights (better models get more influence)\n",
            "\n",
            "Key Characteristics:\n",
            "  ‚úÖ Models trained SEQUENTIALLY (one after another, dependent)\n",
            "  ‚úÖ Reduces BIAS (systematic errors, underfitting)\n",
            "  ‚úÖ Adaptive learning: Each model learns from previous mistakes\n",
            "  ‚úÖ Focused: Later models specialize on hard examples\n",
            "\n",
            "Boosting Types:\n",
            "  1. AdaBoost: Original, gives more weight to mistakes\n",
            "  2. Gradient Boosting: Uses gradient descent to minimize errors\n",
            "  3. XGBoost: Optimized, very popular, handles missing values\n",
            "  4. LightGBM: Fast, good for large datasets\n",
            "  5. CatBoost: Excellent with categorical features\n",
            "\n",
            "Example: AdaBoost\n",
            "  - Model 1: Classifies 80% correctly, 20% wrong\n",
            "  - Model 2: Focuses on the 20% mistakes\n",
            "  - Model 3: Focuses on remaining mistakes\n",
            "  - Final: Weighted combination of all models\n",
            "\n",
            "When to Use:\n",
            "  ‚úÖ High bias models (underfitting)\n",
            "  ‚úÖ Want maximum accuracy\n",
            "  ‚úÖ Can wait for sequential training\n",
            "  ‚úÖ Small to medium datasets\n",
            "  ‚úÖ Need to handle complex patterns\n",
            "\n",
            "\n",
            "======================================================================\n",
            "5. TYPE 3: STACKING (Stacked Generalization)\n",
            "======================================================================\n",
            "\n",
            "Definition: Train a \"meta-learner\" that learns how to best combine\n",
            "            predictions from multiple base models\n",
            "\n",
            "How It Works:\n",
            "  1. Train multiple BASE models (different algorithms)\n",
            "     - Example: Logistic Regression, Random Forest, SVM\n",
            "  2. Get predictions from each base model\n",
            "  3. Use predictions as FEATURES for META-LEARNER\n",
            "  4. Train meta-learner to learn optimal combination\n",
            "  5. Final prediction = meta-learner's output\n",
            "\n",
            "Key Characteristics:\n",
            "  ‚úÖ Two levels: Base models (Level 1) + Meta-learner (Level 2)\n",
            "  ‚úÖ Meta-learner learns HOW to combine base models\n",
            "  ‚úÖ Can use different algorithms for base and meta\n",
            "  ‚úÖ More flexible than simple voting/averaging\n",
            "\n",
            "Example Structure:\n",
            "  Level 1 (Base Models):\n",
            "    - Model A: Logistic Regression ‚Üí Prediction A\n",
            "    - Model B: Random Forest ‚Üí Prediction B\n",
            "    - Model C: SVM ‚Üí Prediction C\n",
            "\n",
            "  Level 2 (Meta-Learner):\n",
            "    - Input: [Prediction A, Prediction B, Prediction C]\n",
            "    - Algorithm: Linear Regression or Neural Network\n",
            "    - Output: Final optimized prediction\n",
            "\n",
            "Why It Works:\n",
            "  - Different models have different strengths\n",
            "  - Meta-learner learns which model to trust for which cases\n",
            "  - More sophisticated than simple averaging\n",
            "\n",
            "When to Use:\n",
            "  ‚úÖ Have diverse base models (different algorithms)\n",
            "  ‚úÖ Want optimal combination (not just average)\n",
            "  ‚úÖ Have enough data for meta-learner training\n",
            "  ‚úÖ Need best possible accuracy\n",
            "  ‚úÖ Willing to invest in complexity\n",
            "\n",
            "\n",
            "======================================================================\n",
            "6. TYPE 4: VOTING (Hard & Soft Voting)\n",
            "======================================================================\n",
            "\n",
            "Definition: Multiple models vote on the final prediction\n",
            "\n",
            "Hard Voting:\n",
            "  - Each model predicts a CLASS (category)\n",
            "  - Final prediction = MAJORITY VOTE (most common class)\n",
            "  - Example: 5 models predict [A, A, A, B, B] ‚Üí Final = A (3 votes)\n",
            "\n",
            "Soft Voting:\n",
            "  - Each model outputs PROBABILITIES\n",
            "  - Final prediction = AVERAGE probabilities, pick highest\n",
            "  - Example:\n",
            "      Model 1: [0.7, 0.2, 0.1] for classes [A, B, C]\n",
            "      Model 2: [0.6, 0.3, 0.1]\n",
            "      Model 3: [0.5, 0.4, 0.1]\n",
            "      Average: [0.6, 0.3, 0.1] ‚Üí Final = A (highest probability)\n",
            "\n",
            "Key Characteristics:\n",
            "  ‚úÖ Simplest ensemble method\n",
            "  ‚úÖ No training of combination (just vote/average)\n",
            "  ‚úÖ Works well with diverse models\n",
            "  ‚úÖ Fast and interpretable\n",
            "  ‚úÖ Soft voting usually better (uses probability information)\n",
            "\n",
            "Requirements:\n",
            "  - Hard Voting: Models must predict classes\n",
            "  - Soft Voting: Models must output probabilities\n",
            "\n",
            "When to Use:\n",
            "  ‚úÖ Want simple ensemble (easiest to implement)\n",
            "  ‚úÖ Have diverse models (different algorithms)\n",
            "  ‚úÖ Need fast predictions\n",
            "  ‚úÖ Classification tasks\n",
            "  ‚úÖ Soft voting preferred when models output probabilities\n",
            "\n",
            "\n",
            "======================================================================\n",
            "7. TYPE 5: BLENDING\n",
            "======================================================================\n",
            "\n",
            "Definition: Manually combine predictions with fixed weights or simple rules\n",
            "            (Simpler version of stacking)\n",
            "\n",
            "How It Works:\n",
            "  1. Train multiple models\n",
            "  2. Get predictions from each model\n",
            "  3. Combine with fixed weights or simple formula\n",
            "  4. No meta-learner training (unlike stacking)\n",
            "\n",
            "Key Characteristics:\n",
            "  ‚úÖ Simpler than stacking (no meta-learner to train)\n",
            "  ‚úÖ Weights can be learned or set manually\n",
            "  ‚úÖ Faster than stacking\n",
            "  ‚ö†Ô∏è  Less flexible than stacking\n",
            "\n",
            "Example:\n",
            "  Model 1 (Random Forest): Prediction = 0.7\n",
            "  Model 2 (SVM): Prediction = 0.8\n",
            "  Model 3 (Logistic Regression): Prediction = 0.6\n",
            "\n",
            "  Blending with weights [0.4, 0.4, 0.2]:\n",
            "    Final = 0.4*0.7 + 0.4*0.8 + 0.2*0.6 = 0.72\n",
            "\n",
            "When to Use:\n",
            "  ‚úÖ Want something between voting and stacking\n",
            "  ‚úÖ Have domain knowledge about model weights\n",
            "  ‚úÖ Need faster training than stacking\n",
            "  ‚úÖ Want more control than voting\n",
            "\n",
            "\n",
            "======================================================================\n",
            "8. COMPARISON: WHEN TO USE WHICH METHOD?\n",
            "======================================================================\n",
            "\n",
            "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
            "‚îÇ   Method    ‚îÇ   Training   ‚îÇ   Reduces    ‚îÇ   Best For   ‚îÇ   Examples   ‚îÇ\n",
            "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
            "‚îÇ  BAGGING    ‚îÇ  Parallel    ‚îÇ  Variance    ‚îÇ  Overfitting ‚îÇ Random Forest‚îÇ\n",
            "‚îÇ             ‚îÇ  (fast)      ‚îÇ  (noise)     ‚îÇ  High var    ‚îÇ Extra Trees  ‚îÇ\n",
            "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
            "‚îÇ  BOOSTING   ‚îÇ  Sequential  ‚îÇ  Bias        ‚îÇ  Underfitting‚îÇ XGBoost      ‚îÇ\n",
            "‚îÇ             ‚îÇ  (slow)      ‚îÇ  (errors)    ‚îÇ  High bias   ‚îÇ LightGBM     ‚îÇ\n",
            "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
            "‚îÇ  STACKING   ‚îÇ  Two-level   ‚îÇ  Both        ‚îÇ  Max accuracy‚îÇ Custom stack ‚îÇ\n",
            "‚îÇ             ‚îÇ  (complex)   ‚îÇ              ‚îÇ  Diverse mods‚îÇ              ‚îÇ\n",
            "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
            "‚îÇ  VOTING     ‚îÇ  Independent ‚îÇ  Errors      ‚îÇ  Simple      ‚îÇ Voting       ‚îÇ\n",
            "‚îÇ             ‚îÇ  (simple)    ‚îÇ              ‚îÇ  Fast        ‚îÇ Classifier   ‚îÇ\n",
            "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
            "‚îÇ  BLENDING   ‚îÇ  Independent ‚îÇ  Errors      ‚îÇ  Medium      ‚îÇ Weighted avg ‚îÇ\n",
            "‚îÇ             ‚îÇ  (medium)    ‚îÇ              ‚îÇ  complexity  ‚îÇ              ‚îÇ\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
            "\n",
            "Key Differences:\n",
            "  Training Speed: Bagging = Voting = Blending > Stacking > Boosting\n",
            "  Complexity: Voting < Blending < Bagging < Boosting < Stacking\n",
            "  Accuracy Potential: Voting < Blending < Bagging < Boosting < Stacking\n",
            "  Interpretability: Voting > Blending > Bagging > Boosting > Stacking\n",
            "\n",
            "\n",
            "======================================================================\n",
            "9. KEY CONCEPTS IN ENSEMBLE LEARNING\n",
            "======================================================================\n",
            "\n",
            "1. DIVERSITY (Critical for Good Ensembles)\n",
            "   - Models should make DIFFERENT mistakes\n",
            "   - If all models agree, ensemble = single model\n",
            "   - Diversity sources:\n",
            "     * Different algorithms (Logistic Regression vs Random Forest)\n",
            "     * Different features (subset of features)\n",
            "     * Different data (bootstrap samples)\n",
            "     * Different hyperparameters\n",
            "\n",
            "2. BIAS vs VARIANCE\n",
            "   - Bias: Systematic error (underfitting)\n",
            "   - Variance: Sensitivity to data changes (overfitting)\n",
            "   - Bagging reduces VARIANCE\n",
            "   - Boosting reduces BIAS\n",
            "   - Stacking can reduce BOTH\n",
            "\n",
            "3. OVERFITTING IN ENSEMBLES\n",
            "   - Ensembles can still overfit!\n",
            "   - Too many models = memorization\n",
            "   - Need validation to tune ensemble size\n",
            "   - Cross-validation important\n",
            "\n",
            "4. MODEL CORRELATION\n",
            "   - Low correlation between models = Better ensemble\n",
            "   - High correlation = Less benefit\n",
            "   - Diversity reduces correlation\n",
            "\n",
            "5. COMBINATION STRATEGIES\n",
            "   - Average: Simple, works well\n",
            "   - Weighted Average: Better models get more weight\n",
            "   - Voting: For classification\n",
            "   - Stacking: Learned combination (most flexible)\n",
            "\n",
            "\n",
            "======================================================================\n",
            "10. PRACTICAL TIPS FOR ENSEMBLE LEARNING\n",
            "======================================================================\n",
            "\n",
            "‚úÖ Start Simple:\n",
            "   - Try Voting Classifier first (easiest)\n",
            "   - Then try Bagging (Random Forest)\n",
            "   - Then Boosting (XGBoost)\n",
            "   - Finally Stacking (if needed)\n",
            "\n",
            "‚úÖ Ensure Diversity:\n",
            "   - Use different algorithms\n",
            "   - Use different features\n",
            "   - Use different hyperparameters\n",
            "   - Don't use identical models\n",
            "\n",
            "‚úÖ Validate Properly:\n",
            "   - Use cross-validation\n",
            "   - Separate validation set for meta-learner (stacking)\n",
            "   - Watch for overfitting\n",
            "\n",
            "‚úÖ Consider Trade-offs:\n",
            "   - Accuracy vs Speed\n",
            "   - Complexity vs Interpretability\n",
            "   - Training time vs Prediction time\n",
            "\n",
            "‚úÖ Common Mistakes:\n",
            "   ‚ùå Using identical models (no diversity)\n",
            "   ‚ùå Too many models (overfitting)\n",
            "   ‚ùå Not validating properly\n",
            "   ‚ùå Ignoring computational cost\n",
            "   ‚ùå Using ensemble when single model is sufficient\n",
            "\n",
            "\n",
            "======================================================================\n",
            "ENSEMBLE LEARNING GUIDE COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "Next Steps:\n",
            "1. Try implementing different ensemble methods\n",
            "2. Compare their performance\n",
            "3. Understand when each works best\n",
            "4. Practice with real datasets\n",
            "\n",
            "Remember: Ensemble learning is a powerful tool, but not always necessary.\n",
            "          Sometimes a well-tuned single model is sufficient!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ENSEMBLE LEARNING - COMPREHENSIVE GUIDE\n",
        "# ============================================================================\n",
        "# This cell provides a complete explanation of ensemble learning methods\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ENSEMBLE LEARNING - COMPLETE GUIDE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============================================================================\n",
        "# WHAT IS ENSEMBLE LEARNING?\n",
        "# ============================================================================\n",
        "# Ensemble Learning: Combining multiple machine learning models to create\n",
        "# a more powerful and accurate prediction system.\n",
        "#\n",
        "# Core Philosophy: \"Wisdom of the Crowd\"\n",
        "# - Multiple opinions are better than one\n",
        "# - Different models catch different patterns\n",
        "# - Errors cancel out when models disagree\n",
        "# - Similar to asking multiple experts and taking their consensus\n",
        "#\n",
        "# Analogy:\n",
        "#   Single Model: Asking one doctor for diagnosis\n",
        "#   Ensemble: Asking 10 doctors and taking majority vote\n",
        "#   Result: More reliable and accurate diagnosis\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"1. WHAT IS ENSEMBLE LEARNING?\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "Ensemble Learning = Combining multiple models for better predictions\n",
        "\n",
        "Key Idea: \"Wisdom of the Crowd\"\n",
        "- Multiple models ‚Üí Multiple perspectives\n",
        "- Errors cancel out ‚Üí More robust predictions\n",
        "- Different models catch different patterns ‚Üí Better coverage\n",
        "\n",
        "Real-World Analogy:\n",
        "  Single Model: One weather forecaster\n",
        "  Ensemble: 100 weather forecasters voting\n",
        "  Result: More accurate weather prediction\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# WHY USE ENSEMBLE LEARNING?\n",
        "# ============================================================================\n",
        "# Benefits:\n",
        "# 1. Better Accuracy: Often 5-20% improvement over single models\n",
        "# 2. More Robust: Less sensitive to outliers and noise\n",
        "# 3. Reduces Overfitting: Multiple models prevent memorization\n",
        "# 4. Handles Complexity: Captures patterns single models miss\n",
        "# 5. Error Reduction: Mistakes from one model offset by others\n",
        "#\n",
        "# Trade-offs:\n",
        "# - More computational cost (train multiple models)\n",
        "# - More complex to understand and explain\n",
        "# - Takes longer to train and predict\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"2. WHY USE ENSEMBLE LEARNING?\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "Benefits:\n",
        "‚úÖ Better Accuracy: 5-20% improvement over single models\n",
        "‚úÖ More Robust: Less sensitive to outliers and noise\n",
        "‚úÖ Reduces Overfitting: Multiple models prevent memorization\n",
        "‚úÖ Handles Complexity: Captures patterns single models miss\n",
        "‚úÖ Error Reduction: Mistakes cancel out\n",
        "\n",
        "Trade-offs:\n",
        "‚ö†Ô∏è  More computational cost (train multiple models)\n",
        "‚ö†Ô∏è  More complex to understand and explain\n",
        "‚ö†Ô∏è  Takes longer to train and predict\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# TYPE 1: BAGGING (Bootstrap Aggregating)\n",
        "# ============================================================================\n",
        "# Bagging: Train multiple models on different random subsets of data,\n",
        "#          then average/vote their predictions\n",
        "#\n",
        "# How It Works:\n",
        "# 1. Create multiple bootstrap samples (random sampling with replacement)\n",
        "# 2. Train a model on each sample (same algorithm, different data)\n",
        "# 3. For prediction: Average (regression) or Vote (classification)\n",
        "#\n",
        "# Key Characteristics:\n",
        "# - Models trained in PARALLEL (independent)\n",
        "# - Reduces VARIANCE (model sensitivity to data)\n",
        "# - Works well with high-variance models (Decision Trees, Neural Networks)\n",
        "# - Each model sees ~63% of data (bootstrap sampling)\n",
        "#\n",
        "# Examples:\n",
        "# - Random Forest: Bagging of Decision Trees\n",
        "# - Extra Trees: More randomized version of Random Forest\n",
        "#\n",
        "# When to Use:\n",
        "# - High variance models (overfitting)\n",
        "# - Large datasets\n",
        "# - Want parallel training\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"3. TYPE 1: BAGGING (Bootstrap Aggregating)\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "Definition: Train multiple models on different random data subsets,\n",
        "            then combine their predictions\n",
        "\n",
        "How It Works:\n",
        "  1. Create bootstrap samples (random sampling WITH replacement)\n",
        "  2. Train model on each sample (same algorithm, different data)\n",
        "  3. Predict: Average (regression) or Majority Vote (classification)\n",
        "\n",
        "Key Characteristics:\n",
        "  ‚úÖ Models trained in PARALLEL (independent, can train simultaneously)\n",
        "  ‚úÖ Reduces VARIANCE (less sensitive to data changes)\n",
        "  ‚úÖ Works well with high-variance models (Decision Trees, Neural Nets)\n",
        "  ‚úÖ Each model sees ~63% of data (bootstrap sampling)\n",
        "\n",
        "Bootstrap Sampling:\n",
        "  - Random sampling WITH replacement\n",
        "  - Sample size = original dataset size\n",
        "  - Some samples appear multiple times, some not at all\n",
        "  - Each sample is different ‚Üí different models\n",
        "\n",
        "Example: Random Forest\n",
        "  - 100 Decision Trees, each on different bootstrap sample\n",
        "  - Final prediction = majority vote of all trees\n",
        "  - More trees = better (but diminishing returns)\n",
        "\n",
        "When to Use:\n",
        "  ‚úÖ High variance models (overfitting)\n",
        "  ‚úÖ Large datasets\n",
        "  ‚úÖ Want parallel training (faster)\n",
        "  ‚úÖ Need robust predictions\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# TYPE 2: BOOSTING\n",
        "# ============================================================================\n",
        "# Boosting: Train models SEQUENTIALLY, where each model learns from\n",
        "#           the mistakes of previous models\n",
        "#\n",
        "# How It Works:\n",
        "# 1. Train first model on all data\n",
        "# 2. Identify mistakes (misclassified/poorly predicted examples)\n",
        "# 3. Train next model focusing on mistakes (give them more weight)\n",
        "# 4. Repeat: Each model corrects previous errors\n",
        "# 5. Combine all models with weights (better models get more weight)\n",
        "#\n",
        "# Key Characteristics:\n",
        "# - Models trained SEQUENTIALLY (one after another)\n",
        "# - Reduces BIAS (systematic errors)\n",
        "# - Each model focuses on previous mistakes\n",
        "# - Adaptive: Learns from errors\n",
        "#\n",
        "# Types of Boosting:\n",
        "# 1. AdaBoost (Adaptive Boosting): Original boosting algorithm\n",
        "# 2. Gradient Boosting: Uses gradient descent to minimize errors\n",
        "# 3. XGBoost: Optimized gradient boosting (very popular)\n",
        "# 4. LightGBM: Fast gradient boosting (Microsoft)\n",
        "# 5. CatBoost: Handles categorical features well\n",
        "#\n",
        "# When to Use:\n",
        "# - High bias models (underfitting)\n",
        "# - Want maximum accuracy\n",
        "# - Can wait for sequential training\n",
        "# - Small to medium datasets\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"4. TYPE 2: BOOSTING\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "Definition: Train models SEQUENTIALLY, each learning from previous mistakes\n",
        "\n",
        "How It Works:\n",
        "  1. Train Model 1 on all data\n",
        "  2. Identify mistakes (misclassified examples)\n",
        "  3. Train Model 2 focusing on mistakes (give them more weight)\n",
        "  4. Train Model 3 focusing on Model 2's mistakes\n",
        "  5. Continue: Each model corrects previous errors\n",
        "  6. Combine with weights (better models get more influence)\n",
        "\n",
        "Key Characteristics:\n",
        "  ‚úÖ Models trained SEQUENTIALLY (one after another, dependent)\n",
        "  ‚úÖ Reduces BIAS (systematic errors, underfitting)\n",
        "  ‚úÖ Adaptive learning: Each model learns from previous mistakes\n",
        "  ‚úÖ Focused: Later models specialize on hard examples\n",
        "\n",
        "Boosting Types:\n",
        "  1. AdaBoost: Original, gives more weight to mistakes\n",
        "  2. Gradient Boosting: Uses gradient descent to minimize errors\n",
        "  3. XGBoost: Optimized, very popular, handles missing values\n",
        "  4. LightGBM: Fast, good for large datasets\n",
        "  5. CatBoost: Excellent with categorical features\n",
        "\n",
        "Example: AdaBoost\n",
        "  - Model 1: Classifies 80% correctly, 20% wrong\n",
        "  - Model 2: Focuses on the 20% mistakes\n",
        "  - Model 3: Focuses on remaining mistakes\n",
        "  - Final: Weighted combination of all models\n",
        "\n",
        "When to Use:\n",
        "  ‚úÖ High bias models (underfitting)\n",
        "  ‚úÖ Want maximum accuracy\n",
        "  ‚úÖ Can wait for sequential training\n",
        "  ‚úÖ Small to medium datasets\n",
        "  ‚úÖ Need to handle complex patterns\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# TYPE 3: STACKING (Stacked Generalization)\n",
        "# ============================================================================\n",
        "# Stacking: Train a \"meta-learner\" (second-level model) that learns\n",
        "#           how to best combine predictions from multiple \"base learners\"\n",
        "#\n",
        "# How It Works:\n",
        "# 1. Train multiple base models (different algorithms)\n",
        "# 2. Get predictions from each base model\n",
        "# 3. Use these predictions as FEATURES for meta-learner\n",
        "# 4. Train meta-learner to learn optimal combination\n",
        "# 5. Final prediction = meta-learner's prediction\n",
        "#\n",
        "# Key Characteristics:\n",
        "# - Two levels: Base models + Meta-learner\n",
        "# - Meta-learner learns HOW to combine base models\n",
        "# - Can use different algorithms for base and meta\n",
        "# - More flexible than simple voting/averaging\n",
        "#\n",
        "# Example:\n",
        "#   Base Models: Logistic Regression, Random Forest, SVM\n",
        "#   Meta-Learner: Linear Regression (learns weights)\n",
        "#   Final: Meta-learner combines base predictions optimally\n",
        "#\n",
        "# When to Use:\n",
        "# - Have diverse base models\n",
        "# - Want optimal combination\n",
        "# - Have enough data for meta-learner\n",
        "# - Need best possible accuracy\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"5. TYPE 3: STACKING (Stacked Generalization)\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "Definition: Train a \"meta-learner\" that learns how to best combine\n",
        "            predictions from multiple base models\n",
        "\n",
        "How It Works:\n",
        "  1. Train multiple BASE models (different algorithms)\n",
        "     - Example: Logistic Regression, Random Forest, SVM\n",
        "  2. Get predictions from each base model\n",
        "  3. Use predictions as FEATURES for META-LEARNER\n",
        "  4. Train meta-learner to learn optimal combination\n",
        "  5. Final prediction = meta-learner's output\n",
        "\n",
        "Key Characteristics:\n",
        "  ‚úÖ Two levels: Base models (Level 1) + Meta-learner (Level 2)\n",
        "  ‚úÖ Meta-learner learns HOW to combine base models\n",
        "  ‚úÖ Can use different algorithms for base and meta\n",
        "  ‚úÖ More flexible than simple voting/averaging\n",
        "\n",
        "Example Structure:\n",
        "  Level 1 (Base Models):\n",
        "    - Model A: Logistic Regression ‚Üí Prediction A\n",
        "    - Model B: Random Forest ‚Üí Prediction B\n",
        "    - Model C: SVM ‚Üí Prediction C\n",
        "  \n",
        "  Level 2 (Meta-Learner):\n",
        "    - Input: [Prediction A, Prediction B, Prediction C]\n",
        "    - Algorithm: Linear Regression or Neural Network\n",
        "    - Output: Final optimized prediction\n",
        "\n",
        "Why It Works:\n",
        "  - Different models have different strengths\n",
        "  - Meta-learner learns which model to trust for which cases\n",
        "  - More sophisticated than simple averaging\n",
        "\n",
        "When to Use:\n",
        "  ‚úÖ Have diverse base models (different algorithms)\n",
        "  ‚úÖ Want optimal combination (not just average)\n",
        "  ‚úÖ Have enough data for meta-learner training\n",
        "  ‚úÖ Need best possible accuracy\n",
        "  ‚úÖ Willing to invest in complexity\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# TYPE 4: VOTING (Hard & Soft Voting)\n",
        "# ============================================================================\n",
        "# Voting: Simple ensemble where multiple models vote on the prediction\n",
        "#\n",
        "# Hard Voting:\n",
        "# - Each model predicts a class\n",
        "# - Final prediction = majority vote (most common class)\n",
        "# - Example: 3 models predict [A, A, B] ‚Üí Final = A\n",
        "#\n",
        "# Soft Voting:\n",
        "# - Each model outputs probabilities\n",
        "# - Final prediction = average probabilities, then pick highest\n",
        "# - Example: Model 1: [0.7, 0.3], Model 2: [0.6, 0.4] ‚Üí Average: [0.65, 0.35] ‚Üí Class 1\n",
        "#\n",
        "# Key Characteristics:\n",
        "# - Simplest ensemble method\n",
        "# - No training of combination (just vote/average)\n",
        "# - Works well with diverse models\n",
        "# - Fast and interpretable\n",
        "#\n",
        "# When to Use:\n",
        "# - Want simple ensemble\n",
        "# - Have diverse models\n",
        "# - Need fast predictions\n",
        "# - Classification tasks\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"6. TYPE 4: VOTING (Hard & Soft Voting)\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "Definition: Multiple models vote on the final prediction\n",
        "\n",
        "Hard Voting:\n",
        "  - Each model predicts a CLASS (category)\n",
        "  - Final prediction = MAJORITY VOTE (most common class)\n",
        "  - Example: 5 models predict [A, A, A, B, B] ‚Üí Final = A (3 votes)\n",
        "\n",
        "Soft Voting:\n",
        "  - Each model outputs PROBABILITIES\n",
        "  - Final prediction = AVERAGE probabilities, pick highest\n",
        "  - Example:\n",
        "      Model 1: [0.7, 0.2, 0.1] for classes [A, B, C]\n",
        "      Model 2: [0.6, 0.3, 0.1]\n",
        "      Model 3: [0.5, 0.4, 0.1]\n",
        "      Average: [0.6, 0.3, 0.1] ‚Üí Final = A (highest probability)\n",
        "\n",
        "Key Characteristics:\n",
        "  ‚úÖ Simplest ensemble method\n",
        "  ‚úÖ No training of combination (just vote/average)\n",
        "  ‚úÖ Works well with diverse models\n",
        "  ‚úÖ Fast and interpretable\n",
        "  ‚úÖ Soft voting usually better (uses probability information)\n",
        "\n",
        "Requirements:\n",
        "  - Hard Voting: Models must predict classes\n",
        "  - Soft Voting: Models must output probabilities\n",
        "\n",
        "When to Use:\n",
        "  ‚úÖ Want simple ensemble (easiest to implement)\n",
        "  ‚úÖ Have diverse models (different algorithms)\n",
        "  ‚úÖ Need fast predictions\n",
        "  ‚úÖ Classification tasks\n",
        "  ‚úÖ Soft voting preferred when models output probabilities\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# TYPE 5: BLENDING\n",
        "# ============================================================================\n",
        "# Blending: Similar to stacking, but simpler - manually combine\n",
        "#           predictions with fixed weights or simple rules\n",
        "#\n",
        "# How It Works:\n",
        "# 1. Train multiple models\n",
        "# 2. Get predictions from each\n",
        "# 3. Combine with fixed weights or simple formula\n",
        "# 4. No meta-learner training\n",
        "#\n",
        "# Key Characteristics:\n",
        "# - Simpler than stacking (no meta-learner)\n",
        "# - Weights can be learned or set manually\n",
        "# - Faster than stacking\n",
        "# - Less flexible than stacking\n",
        "#\n",
        "# When to Use:\n",
        "# - Want something between voting and stacking\n",
        "# - Have domain knowledge about model weights\n",
        "# - Need faster training than stacking\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"7. TYPE 5: BLENDING\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "Definition: Manually combine predictions with fixed weights or simple rules\n",
        "            (Simpler version of stacking)\n",
        "\n",
        "How It Works:\n",
        "  1. Train multiple models\n",
        "  2. Get predictions from each model\n",
        "  3. Combine with fixed weights or simple formula\n",
        "  4. No meta-learner training (unlike stacking)\n",
        "\n",
        "Key Characteristics:\n",
        "  ‚úÖ Simpler than stacking (no meta-learner to train)\n",
        "  ‚úÖ Weights can be learned or set manually\n",
        "  ‚úÖ Faster than stacking\n",
        "  ‚ö†Ô∏è  Less flexible than stacking\n",
        "\n",
        "Example:\n",
        "  Model 1 (Random Forest): Prediction = 0.7\n",
        "  Model 2 (SVM): Prediction = 0.8\n",
        "  Model 3 (Logistic Regression): Prediction = 0.6\n",
        "  \n",
        "  Blending with weights [0.4, 0.4, 0.2]:\n",
        "    Final = 0.4*0.7 + 0.4*0.8 + 0.2*0.6 = 0.72\n",
        "\n",
        "When to Use:\n",
        "  ‚úÖ Want something between voting and stacking\n",
        "  ‚úÖ Have domain knowledge about model weights\n",
        "  ‚úÖ Need faster training than stacking\n",
        "  ‚úÖ Want more control than voting\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# COMPARISON TABLE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"8. COMPARISON: WHEN TO USE WHICH METHOD?\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ   Method    ‚îÇ   Training   ‚îÇ   Reduces    ‚îÇ   Best For   ‚îÇ   Examples   ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  BAGGING    ‚îÇ  Parallel    ‚îÇ  Variance    ‚îÇ  Overfitting ‚îÇ Random Forest‚îÇ\n",
        "‚îÇ             ‚îÇ  (fast)      ‚îÇ  (noise)     ‚îÇ  High var    ‚îÇ Extra Trees  ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  BOOSTING   ‚îÇ  Sequential  ‚îÇ  Bias        ‚îÇ  Underfitting‚îÇ XGBoost      ‚îÇ\n",
        "‚îÇ             ‚îÇ  (slow)      ‚îÇ  (errors)    ‚îÇ  High bias   ‚îÇ LightGBM     ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  STACKING   ‚îÇ  Two-level   ‚îÇ  Both        ‚îÇ  Max accuracy‚îÇ Custom stack ‚îÇ\n",
        "‚îÇ             ‚îÇ  (complex)   ‚îÇ              ‚îÇ  Diverse mods‚îÇ              ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  VOTING     ‚îÇ  Independent ‚îÇ  Errors      ‚îÇ  Simple      ‚îÇ Voting       ‚îÇ\n",
        "‚îÇ             ‚îÇ  (simple)    ‚îÇ              ‚îÇ  Fast        ‚îÇ Classifier   ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  BLENDING   ‚îÇ  Independent ‚îÇ  Errors      ‚îÇ  Medium      ‚îÇ Weighted avg ‚îÇ\n",
        "‚îÇ             ‚îÇ  (medium)    ‚îÇ              ‚îÇ  complexity  ‚îÇ              ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "Key Differences:\n",
        "  Training Speed: Bagging = Voting = Blending > Stacking > Boosting\n",
        "  Complexity: Voting < Blending < Bagging < Boosting < Stacking\n",
        "  Accuracy Potential: Voting < Blending < Bagging < Boosting < Stacking\n",
        "  Interpretability: Voting > Blending > Bagging > Boosting > Stacking\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# KEY CONCEPTS IN ENSEMBLE LEARNING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"9. KEY CONCEPTS IN ENSEMBLE LEARNING\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "1. DIVERSITY (Critical for Good Ensembles)\n",
        "   - Models should make DIFFERENT mistakes\n",
        "   - If all models agree, ensemble = single model\n",
        "   - Diversity sources:\n",
        "     * Different algorithms (Logistic Regression vs Random Forest)\n",
        "     * Different features (subset of features)\n",
        "     * Different data (bootstrap samples)\n",
        "     * Different hyperparameters\n",
        "\n",
        "2. BIAS vs VARIANCE\n",
        "   - Bias: Systematic error (underfitting)\n",
        "   - Variance: Sensitivity to data changes (overfitting)\n",
        "   - Bagging reduces VARIANCE\n",
        "   - Boosting reduces BIAS\n",
        "   - Stacking can reduce BOTH\n",
        "\n",
        "3. OVERFITTING IN ENSEMBLES\n",
        "   - Ensembles can still overfit!\n",
        "   - Too many models = memorization\n",
        "   - Need validation to tune ensemble size\n",
        "   - Cross-validation important\n",
        "\n",
        "4. MODEL CORRELATION\n",
        "   - Low correlation between models = Better ensemble\n",
        "   - High correlation = Less benefit\n",
        "   - Diversity reduces correlation\n",
        "\n",
        "5. COMBINATION STRATEGIES\n",
        "   - Average: Simple, works well\n",
        "   - Weighted Average: Better models get more weight\n",
        "   - Voting: For classification\n",
        "   - Stacking: Learned combination (most flexible)\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# PRACTICAL TIPS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"10. PRACTICAL TIPS FOR ENSEMBLE LEARNING\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "‚úÖ Start Simple:\n",
        "   - Try Voting Classifier first (easiest)\n",
        "   - Then try Bagging (Random Forest)\n",
        "   - Then Boosting (XGBoost)\n",
        "   - Finally Stacking (if needed)\n",
        "\n",
        "‚úÖ Ensure Diversity:\n",
        "   - Use different algorithms\n",
        "   - Use different features\n",
        "   - Use different hyperparameters\n",
        "   - Don't use identical models\n",
        "\n",
        "‚úÖ Validate Properly:\n",
        "   - Use cross-validation\n",
        "   - Separate validation set for meta-learner (stacking)\n",
        "   - Watch for overfitting\n",
        "\n",
        "‚úÖ Consider Trade-offs:\n",
        "   - Accuracy vs Speed\n",
        "   - Complexity vs Interpretability\n",
        "   - Training time vs Prediction time\n",
        "\n",
        "‚úÖ Common Mistakes:\n",
        "   ‚ùå Using identical models (no diversity)\n",
        "   ‚ùå Too many models (overfitting)\n",
        "   ‚ùå Not validating properly\n",
        "   ‚ùå Ignoring computational cost\n",
        "   ‚ùå Using ensemble when single model is sufficient\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ENSEMBLE LEARNING GUIDE COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "Next Steps:\n",
        "1. Try implementing different ensemble methods\n",
        "2. Compare their performance\n",
        "3. Understand when each works best\n",
        "4. Practice with real datasets\n",
        "\n",
        "Remember: Ensemble learning is a powerful tool, but not always necessary.\n",
        "          Sometimes a well-tuned single model is sufficient!\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "DEEP DIVE: KEY HYPERPARAMETERS & CONCEPTS\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "CONCEPT 1: DECISION TREES - THE BUILDING BLOCKS\n",
            "======================================================================\n",
            "\n",
            "üå≥ WHAT IS A DECISION TREE?\n",
            "\n",
            "A Decision Tree is a flowchart-like structure that makes decisions by asking questions.\n",
            "\n",
            "Example: Predicting if someone will buy a product\n",
            "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
            "  ‚îÇ Is age > 30?            ‚îÇ\n",
            "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
            "              ‚îÇ\n",
            "      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
            "      ‚îÇ               ‚îÇ\n",
            "     YES             NO\n",
            "      ‚îÇ               ‚îÇ\n",
            "  ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê\n",
            "  ‚îÇ Buy   ‚îÇ      ‚îÇ Income‚îÇ\n",
            "  ‚îÇ (80%) ‚îÇ      ‚îÇ > 50K?‚îÇ\n",
            "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò\n",
            "                      ‚îÇ\n",
            "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
            "              ‚îÇ               ‚îÇ\n",
            "             YES             NO\n",
            "              ‚îÇ               ‚îÇ\n",
            "          ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê\n",
            "          ‚îÇ Buy   ‚îÇ      ‚îÇ Don't ‚îÇ\n",
            "          ‚îÇ (60%) ‚îÇ      ‚îÇ Buy   ‚îÇ\n",
            "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ (20%) ‚îÇ\n",
            "                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
            "\n",
            "KEY COMPONENTS:\n",
            "1. Root Node: Top decision point (first question)\n",
            "2. Internal Nodes: Decision points (questions)\n",
            "3. Leaf Nodes: Final predictions (answers)\n",
            "4. Branches: Paths from questions to answers\n",
            "5. Splits: How data is divided at each node\n",
            "\n",
            "HOW IT WORKS:\n",
            "- Starts at root: \"Which feature should I check first?\"\n",
            "- Splits data based on feature value\n",
            "- Repeats for each subset\n",
            "- Stops when criteria met (max depth, min samples, etc.)\n",
            "- Makes prediction at leaf nodes\n",
            "\n",
            "ADVANTAGES:\n",
            "‚úÖ Easy to understand and visualize\n",
            "‚úÖ Handles both numeric and categorical data\n",
            "‚úÖ No feature scaling needed\n",
            "‚úÖ Can capture non-linear relationships\n",
            "\n",
            "DISADVANTAGES:\n",
            "‚ùå Prone to overfitting (memorizes training data)\n",
            "‚ùå Sensitive to small data changes\n",
            "‚ùå Can create biased trees if classes are imbalanced\n",
            "‚ùå May not capture complex relationships well\n",
            "\n",
            "OVERFITTING EXAMPLE:\n",
            "  Training Data: 100% accuracy\n",
            "  Test Data: 60% accuracy\n",
            "  ‚Üí Tree memorized training data too well!\n",
            "\n",
            "SOLUTION: Use multiple trees (Random Forest) or limit tree depth\n",
            "\n",
            "\n",
            "======================================================================\n",
            "CONCEPT 2: RANDOM FOREST - ENSEMBLE OF TREES\n",
            "======================================================================\n",
            "\n",
            "üå≤üå≤üå≤ WHAT IS A RANDOM FOREST?\n",
            "\n",
            "Random Forest = Multiple Decision Trees working together\n",
            "\n",
            "Analogy: \"Wisdom of the Crowd\"\n",
            "  - One person's opinion: Might be wrong\n",
            "  - 100 people's average opinion: Usually more accurate\n",
            "  - Random Forest: 100+ trees voting together\n",
            "\n",
            "HOW RANDOM FOREST WORKS:\n",
            "\n",
            "Step 1: CREATE MULTIPLE DATASETS (Bootstrap Sampling)\n",
            "  Original Data: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "\n",
            "  Tree 1 Dataset: [1, 2, 3, 3, 5, 7, 7, 9, 10, 10]  ‚Üê Random samples with replacement\n",
            "  Tree 2 Dataset: [2, 2, 4, 5, 6, 6, 8, 9, 9, 10]   ‚Üê Different random samples\n",
            "  Tree 3 Dataset: [1, 1, 3, 4, 5, 7, 8, 8, 9, 10]   ‚Üê Another random sample\n",
            "  ... (100 trees total)\n",
            "\n",
            "Step 2: TRAIN EACH TREE ON ITS DATASET\n",
            "  - Each tree sees different data\n",
            "  - Each tree learns different patterns\n",
            "  - Each tree makes different mistakes\n",
            "\n",
            "Step 3: RANDOM FEATURE SELECTION\n",
            "  - At each split, tree only considers random subset of features\n",
            "  - Example: Instead of checking all 10 features, check only 3 random ones\n",
            "  - This creates more diversity between trees\n",
            "\n",
            "Step 4: COMBINE PREDICTIONS (Voting)\n",
            "  New data point: \"Will this person buy?\"\n",
            "\n",
            "  Tree 1 says: \"Buy\" (confidence: 0.7)\n",
            "  Tree 2 says: \"Don't Buy\" (confidence: 0.3)\n",
            "  Tree 3 says: \"Buy\" (confidence: 0.8)\n",
            "  Tree 4 says: \"Buy\" (confidence: 0.6)\n",
            "  ... (100 trees vote)\n",
            "\n",
            "  Final Prediction: \"Buy\" (majority vote: 65 trees say Buy, 35 say Don't)\n",
            "\n",
            "KEY CONCEPTS:\n",
            "\n",
            "1. BOOTSTRAP SAMPLING (Bagging)\n",
            "   - Random sampling WITH replacement\n",
            "   - Each tree sees ~63% of original data\n",
            "   - Some samples appear multiple times\n",
            "   - Some samples never appear (out-of-bag samples)\n",
            "\n",
            "2. FEATURE RANDOMNESS\n",
            "   - At each split, randomly select subset of features\n",
            "   - Default: sqrt(total_features) for classification\n",
            "   - Default: total_features/3 for regression\n",
            "   - Prevents trees from being too similar\n",
            "\n",
            "3. VOTING MECHANISM\n",
            "   - Classification: Majority vote (most common prediction)\n",
            "   - Regression: Average of all predictions\n",
            "\n",
            "WHY RANDOM FOREST WORKS:\n",
            "\n",
            "‚úÖ REDUCES OVERFITTING\n",
            "   - Single tree: Memorizes training data\n",
            "   - Many trees: Errors cancel out\n",
            "\n",
            "‚úÖ HANDLES MISSING DATA\n",
            "   - Can work with incomplete features\n",
            "\n",
            "‚úÖ FEATURE IMPORTANCE\n",
            "   - Can tell which features matter most\n",
            "\n",
            "‚úÖ ROBUST TO OUTLIERS\n",
            "   - Multiple trees average out extreme predictions\n",
            "\n",
            "EXAMPLE: Predicting House Price\n",
            "  Tree 1: Focuses on size ‚Üí Predicts $300K\n",
            "  Tree 2: Focuses on location ‚Üí Predicts $350K\n",
            "  Tree 3: Focuses on age ‚Üí Predicts $280K\n",
            "  Tree 4: Focuses on bedrooms ‚Üí Predicts $320K\n",
            "  ... (100 trees)\n",
            "\n",
            "  Final: Average = $310K (more accurate than any single tree)\n",
            "\n",
            "\n",
            "======================================================================\n",
            "CONCEPT 3: N_ESTIMATORS - NUMBER OF TREES\n",
            "======================================================================\n",
            "\n",
            "üî¢ WHAT IS N_ESTIMATORS?\n",
            "\n",
            "n_estimators = Number of trees in the forest/ensemble\n",
            "\n",
            "Random Forest: n_estimators = 100 means 100 decision trees\n",
            "XGBoost: n_estimators = 100 means 100 boosting rounds (trees)\n",
            "\n",
            "HOW IT AFFECTS PERFORMANCE:\n",
            "\n",
            "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
            "‚îÇ n_estimators‚îÇ Accuracy      ‚îÇ Training Time‚îÇ Overfitting   ‚îÇ\n",
            "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
            "‚îÇ 10          ‚îÇ Lower         ‚îÇ Fast         ‚îÇ Less risk    ‚îÇ\n",
            "‚îÇ 50          ‚îÇ Medium        ‚îÇ Medium       ‚îÇ Medium risk  ‚îÇ\n",
            "‚îÇ 100         ‚îÇ Good          ‚îÇ Slower       ‚îÇ Low risk     ‚îÇ\n",
            "‚îÇ 500         ‚îÇ Better        ‚îÇ Slow         ‚îÇ Very low risk‚îÇ\n",
            "‚îÇ 1000        ‚îÇ Best (often) ‚îÇ Very slow    ‚îÇ Minimal risk ‚îÇ\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
            "\n",
            "TRADE-OFFS:\n",
            "\n",
            "1. MORE TREES (Higher n_estimators)\n",
            "   ‚úÖ Better accuracy (usually)\n",
            "   ‚úÖ More stable predictions\n",
            "   ‚úÖ Less overfitting\n",
            "   ‚ùå Longer training time\n",
            "   ‚ùå More memory usage\n",
            "   ‚ùå Diminishing returns (1000 trees not always much better than 500)\n",
            "\n",
            "2. FEWER TREES (Lower n_estimators)\n",
            "   ‚úÖ Faster training\n",
            "   ‚úÖ Less memory\n",
            "   ‚úÖ Quicker predictions\n",
            "   ‚ùå May be less accurate\n",
            "   ‚ùå More variance in predictions\n",
            "   ‚ùå Higher risk of underfitting\n",
            "\n",
            "DIMINISHING RETURNS:\n",
            "\n",
            "Example: Accuracy vs n_estimators\n",
            "  n_estimators=10:  75% accuracy\n",
            "  n_estimators=50:   82% accuracy  (+7%)\n",
            "  n_estimators=100:  85% accuracy  (+3%)\n",
            "  n_estimators=200:  86% accuracy  (+1%)\n",
            "  n_estimators=500:  86.5% accuracy (+0.5%)\n",
            "  n_estimators=1000: 86.6% accuracy (+0.1%)\n",
            "\n",
            "  ‚Üí After 200 trees, adding more trees doesn't help much!\n",
            "\n",
            "HOW TO CHOOSE N_ESTIMATORS:\n",
            "\n",
            "1. START WITH DEFAULT (100)\n",
            "   - Good balance of accuracy and speed\n",
            "   - Works well for most problems\n",
            "\n",
            "2. IF ACCURACY IS LOW\n",
            "   - Try increasing: 200, 500, 1000\n",
            "   - Monitor if accuracy improves\n",
            "   - Stop when improvement is minimal\n",
            "\n",
            "3. IF TRAINING IS TOO SLOW\n",
            "   - Try decreasing: 50, 25, 10\n",
            "   - Check if accuracy drops significantly\n",
            "   - Find sweet spot between speed and accuracy\n",
            "\n",
            "4. USE CROSS-VALIDATION\n",
            "   - Test different values: [10, 50, 100, 200, 500]\n",
            "   - Choose value with best validation score\n",
            "   - Consider training time in decision\n",
            "\n",
            "PRACTICAL EXAMPLE:\n",
            "\n",
            "Random Forest with different n_estimators:\n",
            "  n_estimators=10:  Training time: 2 seconds,  Accuracy: 78%\n",
            "  n_estimators=100: Training time: 20 seconds, Accuracy: 85%\n",
            "  n_estimators=500: Training time: 100 seconds, Accuracy: 86%\n",
            "\n",
            "  ‚Üí n_estimators=100 is best choice (good accuracy, reasonable time)\n",
            "\n",
            "\n",
            "======================================================================\n",
            "CONCEPT 4: LEARNING RATE - STEP SIZE IN BOOSTING\n",
            "======================================================================\n",
            "\n",
            "üìà WHAT IS LEARNING RATE?\n",
            "\n",
            "Learning Rate = How much each tree contributes to the final prediction\n",
            "\n",
            "Analogy: Learning to ride a bike\n",
            "  - High learning rate: Big corrections ‚Üí Might overshoot, fall down\n",
            "  - Low learning rate: Small corrections ‚Üí Takes longer, but smoother\n",
            "  - Optimal learning rate: Just right ‚Üí Learn efficiently without falling\n",
            "\n",
            "HOW IT WORKS IN BOOSTING (XGBoost, Gradient Boosting):\n",
            "\n",
            "Sequential Learning Process:\n",
            "  Step 1: Train Tree 1 ‚Üí Makes prediction\n",
            "  Step 2: Calculate errors (what Tree 1 got wrong)\n",
            "  Step 3: Train Tree 2 to fix Tree 1's errors\n",
            "  Step 4: Combine: Final = Tree1 + (learning_rate √ó Tree2)\n",
            "  Step 5: Calculate remaining errors\n",
            "  Step 6: Train Tree 3 to fix remaining errors\n",
            "  Step 7: Combine: Final = Tree1 + (lr √ó Tree2) + (lr √ó Tree3)\n",
            "  ... (repeat for n_estimators trees)\n",
            "\n",
            "MATHEMATICAL VIEW:\n",
            "\n",
            "Final Prediction = Tree1 + (lr √ó Tree2) + (lr √ó Tree3) + ... + (lr √ó TreeN)\n",
            "\n",
            "Example with learning_rate = 0.1:\n",
            "  Tree 1 prediction: 100\n",
            "  Tree 2 correction: +20 (fixes some errors)\n",
            "  Tree 3 correction: +15 (fixes more errors)\n",
            "  Tree 4 correction: +10 (fixes remaining errors)\n",
            "\n",
            "  Final = 100 + (0.1 √ó 20) + (0.1 √ó 15) + (0.1 √ó 10)\n",
            "        = 100 + 2 + 1.5 + 1\n",
            "        = 104.5\n",
            "\n",
            "Example with learning_rate = 0.5 (higher):\n",
            "  Final = 100 + (0.5 √ó 20) + (0.5 √ó 15) + (0.5 √ó 10)\n",
            "        = 100 + 10 + 7.5 + 5\n",
            "        = 122.5  ‚Üê Bigger steps, might overshoot!\n",
            "\n",
            "LEARNING RATE VALUES:\n",
            "\n",
            "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
            "‚îÇ Learning Rate‚îÇ Step Size    ‚îÇ Training     ‚îÇ Overfitting  ‚îÇ\n",
            "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
            "‚îÇ 0.01         ‚îÇ Very small   ‚îÇ Slow         ‚îÇ Low risk     ‚îÇ\n",
            "‚îÇ 0.1          ‚îÇ Small        ‚îÇ Medium       ‚îÇ Low risk     ‚îÇ\n",
            "‚îÇ 0.3          ‚îÇ Medium       ‚îÇ Fast         ‚îÇ Medium risk  ‚îÇ\n",
            "‚îÇ 0.5          ‚îÇ Large        ‚îÇ Very fast    ‚îÇ High risk    ‚îÇ\n",
            "‚îÇ 1.0          ‚îÇ Very large   ‚îÇ Fastest      ‚îÇ Very high    ‚îÇ\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
            "\n",
            "TRADE-OFFS:\n",
            "\n",
            "1. HIGH LEARNING RATE (0.3 - 1.0)\n",
            "   ‚úÖ Faster convergence (fewer trees needed)\n",
            "   ‚úÖ Quicker training\n",
            "   ‚ùå May overshoot optimal solution\n",
            "   ‚ùå Higher risk of overfitting\n",
            "   ‚ùå Less stable training\n",
            "\n",
            "2. LOW LEARNING RATE (0.01 - 0.1)\n",
            "   ‚úÖ More stable training\n",
            "   ‚úÖ Better final accuracy (usually)\n",
            "   ‚úÖ Lower risk of overfitting\n",
            "   ‚ùå Slower convergence (needs more trees)\n",
            "   ‚ùå Longer training time\n",
            "\n",
            "LEARNING RATE vs N_ESTIMATORS:\n",
            "\n",
            "These two hyperparameters work together!\n",
            "\n",
            "Example 1: High LR, Few Trees\n",
            "  learning_rate = 0.5, n_estimators = 50\n",
            "  ‚Üí Fast training, but may overfit\n",
            "\n",
            "Example 2: Low LR, Many Trees\n",
            "  learning_rate = 0.01, n_estimators = 1000\n",
            "  ‚Üí Slow training, but very accurate\n",
            "\n",
            "Example 3: Balanced (Recommended)\n",
            "  learning_rate = 0.1, n_estimators = 100\n",
            "  ‚Üí Good balance of speed and accuracy\n",
            "\n",
            "RULE OF THUMB:\n",
            "  - If you increase learning_rate ‚Üí You can decrease n_estimators\n",
            "  - If you decrease learning_rate ‚Üí You should increase n_estimators\n",
            "  - Common combination: lr=0.1, n_estimators=100\n",
            "\n",
            "VISUALIZATION:\n",
            "\n",
            "Learning Rate = 0.1 (Small Steps):\n",
            "  Start: 50% accuracy\n",
            "  Tree 1: 55% accuracy (+5%)\n",
            "  Tree 2: 58% accuracy (+3%)\n",
            "  Tree 3: 60% accuracy (+2%)\n",
            "  Tree 4: 61% accuracy (+1%)\n",
            "  ... (gradual improvement, smooth curve)\n",
            "\n",
            "Learning Rate = 0.5 (Big Steps):\n",
            "  Start: 50% accuracy\n",
            "  Tree 1: 65% accuracy (+15%)\n",
            "  Tree 2: 72% accuracy (+7%)\n",
            "  Tree 3: 75% accuracy (+3%)\n",
            "  Tree 4: 76% accuracy (+1%)\n",
            "  ... (fast improvement, but may overshoot)\n",
            "\n",
            "HOW TO CHOOSE LEARNING RATE:\n",
            "\n",
            "1. START WITH DEFAULT (0.1)\n",
            "   - Works well for most problems\n",
            "   - Good starting point\n",
            "\n",
            "2. IF TRAINING IS TOO SLOW\n",
            "   - Try increasing: 0.2, 0.3\n",
            "   - Monitor for overfitting\n",
            "   - May need to reduce n_estimators\n",
            "\n",
            "3. IF OVERFITTING OCCURS\n",
            "   - Try decreasing: 0.05, 0.01\n",
            "   - May need to increase n_estimators\n",
            "   - Usually improves generalization\n",
            "\n",
            "4. USE GRID SEARCH\n",
            "   - Test combinations: [0.01, 0.1, 0.3] √ó [50, 100, 200]\n",
            "   - Find best combination\n",
            "   - Consider both accuracy and training time\n",
            "\n",
            "PRACTICAL EXAMPLE:\n",
            "\n",
            "XGBoost with different learning rates:\n",
            "  lr=0.01, n_estimators=1000: Accuracy: 88%, Time: 300s\n",
            "  lr=0.1,  n_estimators=100:  Accuracy: 87%, Time: 30s\n",
            "  lr=0.5,  n_estimators=50:   Accuracy: 85%, Time: 15s\n",
            "\n",
            "  ‚Üí lr=0.1, n_estimators=100 is best (good accuracy, reasonable time)\n",
            "\n",
            "\n",
            "======================================================================\n",
            "CONCEPT 5: MAX_DEPTH - TREE COMPLEXITY\n",
            "======================================================================\n",
            "\n",
            "üå≥ WHAT IS MAX_DEPTH?\n",
            "\n",
            "max_depth = Maximum number of levels (depth) a tree can have\n",
            "\n",
            "Example: max_depth = 3\n",
            "  Level 0 (Root):        [Is age > 30?]\n",
            "                           /         \\\n",
            "  Level 1:          [YES]           [NO]\n",
            "                      /  \\            /  \\\n",
            "  Level 2:      [Income] [Buy]   [Income] [Don't]\n",
            "                 /  \\    (leaf)   /  \\     (leaf)\n",
            "  Level 3:   [Buy] [Don't]    [Buy] [Don't]\n",
            "            (leaf) (leaf)    (leaf) (leaf)\n",
            "\n",
            "HOW IT AFFECTS PERFORMANCE:\n",
            "\n",
            "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
            "‚îÇ max_depth ‚îÇ Complexity   ‚îÇ Overfitting  ‚îÇ Training    ‚îÇ\n",
            "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
            "‚îÇ 1         ‚îÇ Very simple  ‚îÇ Low risk     ‚îÇ Fast        ‚îÇ\n",
            "‚îÇ 3         ‚îÇ Simple       ‚îÇ Low risk     ‚îÇ Fast        ‚îÇ\n",
            "‚îÇ 5         ‚îÇ Medium       ‚îÇ Medium risk  ‚îÇ Medium      ‚îÇ\n",
            "‚îÇ 10        ‚îÇ Complex      ‚îÇ High risk    ‚îÇ Slower      ‚îÇ\n",
            "‚îÇ None      ‚îÇ Very complex  ‚îÇ Very high    ‚îÇ Slow        ‚îÇ\n",
            "‚îÇ (unlimited)‚îÇ             ‚îÇ              ‚îÇ             ‚îÇ\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
            "\n",
            "TRADE-OFFS:\n",
            "\n",
            "1. DEEP TREES (High max_depth)\n",
            "   ‚úÖ Can capture complex patterns\n",
            "   ‚úÖ More detailed decisions\n",
            "   ‚ùå Higher risk of overfitting\n",
            "   ‚ùå Longer training time\n",
            "   ‚ùå Harder to interpret\n",
            "\n",
            "2. SHALLOW TREES (Low max_depth)\n",
            "   ‚úÖ Faster training\n",
            "   ‚úÖ Less overfitting\n",
            "   ‚úÖ Easier to interpret\n",
            "   ‚ùå May miss complex patterns\n",
            "   ‚ùå May underfit (too simple)\n",
            "\n",
            "DEFAULT VALUES:\n",
            "  - Random Forest: Usually None (unlimited) or very high\n",
            "  - XGBoost: Usually 6 or None\n",
            "  - Reason: Ensemble methods can handle deeper trees better\n",
            "\n",
            "PRACTICAL EXAMPLE:\n",
            "\n",
            "Random Forest with different max_depth:\n",
            "  max_depth=3:  Accuracy: 80%, Training: 5s,  Overfitting: Low\n",
            "  max_depth=5:  Accuracy: 85%, Training: 8s,  Overfitting: Medium\n",
            "  max_depth=10: Accuracy: 86%, Training: 15s, Overfitting: High\n",
            "  max_depth=None: Accuracy: 87%, Training: 20s, Overfitting: Very High\n",
            "\n",
            "  ‚Üí max_depth=5 or 10 is often best (good accuracy, manageable overfitting)\n",
            "\n",
            "\n",
            "======================================================================\n",
            "CONCEPT 6: RELATIONSHIP BETWEEN HYPERPARAMETERS\n",
            "======================================================================\n",
            "\n",
            "üîó HOW HYPERPARAMETERS WORK TOGETHER:\n",
            "\n",
            "1. N_ESTIMATORS + LEARNING RATE\n",
            "   - More trees can compensate for lower learning rate\n",
            "   - Fewer trees needed with higher learning rate\n",
            "   - Balance: lr=0.1, n_estimators=100 (common default)\n",
            "\n",
            "2. MAX_DEPTH + N_ESTIMATORS\n",
            "   - Deeper trees: May need fewer trees\n",
            "   - Shallower trees: May need more trees\n",
            "   - Reason: Each tree's complexity affects how many you need\n",
            "\n",
            "3. LEARNING RATE + MAX_DEPTH\n",
            "   - High LR + Deep trees: Very fast, but risky\n",
            "   - Low LR + Shallow trees: Slow but safe\n",
            "   - Balanced: Medium LR + Medium depth\n",
            "\n",
            "HYPERPARAMETER TUNING STRATEGY:\n",
            "\n",
            "Step 1: Start with defaults\n",
            "  - n_estimators = 100\n",
            "  - learning_rate = 0.1\n",
            "  - max_depth = 6 (or None)\n",
            "\n",
            "Step 2: If accuracy is low\n",
            "  - Increase n_estimators first (200, 500)\n",
            "  - Then adjust learning_rate if needed\n",
            "  - Finally tune max_depth\n",
            "\n",
            "Step 3: If overfitting occurs\n",
            "  - Decrease max_depth first\n",
            "  - Decrease learning_rate\n",
            "  - Increase n_estimators (to compensate)\n",
            "\n",
            "Step 4: Use Grid Search or Random Search\n",
            "  - Test multiple combinations\n",
            "  - Use cross-validation\n",
            "  - Choose best based on validation score\n",
            "\n",
            "COMMON COMBINATIONS:\n",
            "\n",
            "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
            "‚îÇ Use Case     ‚îÇ n_estimators ‚îÇ learning_rate‚îÇ max_depth    ‚îÇ\n",
            "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
            "‚îÇ Quick test   ‚îÇ 50           ‚îÇ 0.3          ‚îÇ 5            ‚îÇ\n",
            "‚îÇ Default      ‚îÇ 100          ‚îÇ 0.1         ‚îÇ 6            ‚îÇ\n",
            "‚îÇ High accuracy‚îÇ 500          ‚îÇ 0.05         ‚îÇ 8            ‚îÇ\n",
            "‚îÇ Fast training‚îÇ 50           ‚îÇ 0.2          ‚îÇ 4            ‚îÇ\n",
            "‚îÇ Avoid overfit‚îÇ 200          ‚îÇ 0.05         ‚îÇ 4            ‚îÇ\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
            "\n",
            "\n",
            "======================================================================\n",
            "SUMMARY: KEY CONCEPTS\n",
            "======================================================================\n",
            "\n",
            "‚úÖ DECISION TREE: Single model that asks questions to make predictions\n",
            "‚úÖ RANDOM FOREST: Many trees voting together (bagging)\n",
            "‚úÖ N_ESTIMATORS: Number of trees (more = better but slower)\n",
            "‚úÖ LEARNING RATE: Step size in boosting (lower = more stable)\n",
            "‚úÖ MAX_DEPTH: Tree complexity (deeper = more complex but risky)\n",
            "\n",
            "KEY TAKEAWAYS:\n",
            "1. More trees usually = better accuracy (but diminishing returns)\n",
            "2. Lower learning rate = more stable (but needs more trees)\n",
            "3. Deeper trees = more complex (but higher overfitting risk)\n",
            "4. Hyperparameters work together - tune them together\n",
            "5. Use cross-validation to find best combinations\n",
            "\n",
            "NEXT STEPS:\n",
            "- Experiment with different hyperparameter values\n",
            "- Use GridSearchCV for systematic tuning\n",
            "- Monitor training time vs accuracy trade-offs\n",
            "- Understand your data to choose appropriate values\n",
            "\n",
            "\n",
            "======================================================================\n",
            "HYPERPARAMETER GUIDE COMPLETE!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 4: DEEP DIVE INTO KEY HYPERPARAMETERS AND CONCEPTS\n",
        "# ============================================================================\n",
        "# Comprehensive explanation of learning rate, n_estimators, trees, and forests\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DEEP DIVE: KEY HYPERPARAMETERS & CONCEPTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============================================================================\n",
        "# CONCEPT 1: DECISION TREES - THE BUILDING BLOCKS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CONCEPT 1: DECISION TREES - THE BUILDING BLOCKS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "üå≥ WHAT IS A DECISION TREE?\n",
        "\n",
        "A Decision Tree is a flowchart-like structure that makes decisions by asking questions.\n",
        "\n",
        "Example: Predicting if someone will buy a product\n",
        "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "  ‚îÇ Is age > 30?            ‚îÇ\n",
        "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "              ‚îÇ\n",
        "      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "      ‚îÇ               ‚îÇ\n",
        "     YES             NO\n",
        "      ‚îÇ               ‚îÇ\n",
        "  ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê\n",
        "  ‚îÇ Buy   ‚îÇ      ‚îÇ Income‚îÇ\n",
        "  ‚îÇ (80%) ‚îÇ      ‚îÇ > 50K?‚îÇ\n",
        "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò\n",
        "                      ‚îÇ\n",
        "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "              ‚îÇ               ‚îÇ\n",
        "             YES             NO\n",
        "              ‚îÇ               ‚îÇ\n",
        "          ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê\n",
        "          ‚îÇ Buy   ‚îÇ      ‚îÇ Don't ‚îÇ\n",
        "          ‚îÇ (60%) ‚îÇ      ‚îÇ Buy   ‚îÇ\n",
        "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ (20%) ‚îÇ\n",
        "                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "KEY COMPONENTS:\n",
        "1. Root Node: Top decision point (first question)\n",
        "2. Internal Nodes: Decision points (questions)\n",
        "3. Leaf Nodes: Final predictions (answers)\n",
        "4. Branches: Paths from questions to answers\n",
        "5. Splits: How data is divided at each node\n",
        "\n",
        "HOW IT WORKS:\n",
        "- Starts at root: \"Which feature should I check first?\"\n",
        "- Splits data based on feature value\n",
        "- Repeats for each subset\n",
        "- Stops when criteria met (max depth, min samples, etc.)\n",
        "- Makes prediction at leaf nodes\n",
        "\n",
        "ADVANTAGES:\n",
        "‚úÖ Easy to understand and visualize\n",
        "‚úÖ Handles both numeric and categorical data\n",
        "‚úÖ No feature scaling needed\n",
        "‚úÖ Can capture non-linear relationships\n",
        "\n",
        "DISADVANTAGES:\n",
        "‚ùå Prone to overfitting (memorizes training data)\n",
        "‚ùå Sensitive to small data changes\n",
        "‚ùå Can create biased trees if classes are imbalanced\n",
        "‚ùå May not capture complex relationships well\n",
        "\n",
        "OVERFITTING EXAMPLE:\n",
        "  Training Data: 100% accuracy\n",
        "  Test Data: 60% accuracy\n",
        "  ‚Üí Tree memorized training data too well!\n",
        "\n",
        "SOLUTION: Use multiple trees (Random Forest) or limit tree depth\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONCEPT 2: RANDOM FOREST - ENSEMBLE OF TREES\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CONCEPT 2: RANDOM FOREST - ENSEMBLE OF TREES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "üå≤üå≤üå≤ WHAT IS A RANDOM FOREST?\n",
        "\n",
        "Random Forest = Multiple Decision Trees working together\n",
        "\n",
        "Analogy: \"Wisdom of the Crowd\"\n",
        "  - One person's opinion: Might be wrong\n",
        "  - 100 people's average opinion: Usually more accurate\n",
        "  - Random Forest: 100+ trees voting together\n",
        "\n",
        "HOW RANDOM FOREST WORKS:\n",
        "\n",
        "Step 1: CREATE MULTIPLE DATASETS (Bootstrap Sampling)\n",
        "  Original Data: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "  \n",
        "  Tree 1 Dataset: [1, 2, 3, 3, 5, 7, 7, 9, 10, 10]  ‚Üê Random samples with replacement\n",
        "  Tree 2 Dataset: [2, 2, 4, 5, 6, 6, 8, 9, 9, 10]   ‚Üê Different random samples\n",
        "  Tree 3 Dataset: [1, 1, 3, 4, 5, 7, 8, 8, 9, 10]   ‚Üê Another random sample\n",
        "  ... (100 trees total)\n",
        "\n",
        "Step 2: TRAIN EACH TREE ON ITS DATASET\n",
        "  - Each tree sees different data\n",
        "  - Each tree learns different patterns\n",
        "  - Each tree makes different mistakes\n",
        "\n",
        "Step 3: RANDOM FEATURE SELECTION\n",
        "  - At each split, tree only considers random subset of features\n",
        "  - Example: Instead of checking all 10 features, check only 3 random ones\n",
        "  - This creates more diversity between trees\n",
        "\n",
        "Step 4: COMBINE PREDICTIONS (Voting)\n",
        "  New data point: \"Will this person buy?\"\n",
        "  \n",
        "  Tree 1 says: \"Buy\" (confidence: 0.7)\n",
        "  Tree 2 says: \"Don't Buy\" (confidence: 0.3)\n",
        "  Tree 3 says: \"Buy\" (confidence: 0.8)\n",
        "  Tree 4 says: \"Buy\" (confidence: 0.6)\n",
        "  ... (100 trees vote)\n",
        "  \n",
        "  Final Prediction: \"Buy\" (majority vote: 65 trees say Buy, 35 say Don't)\n",
        "\n",
        "KEY CONCEPTS:\n",
        "\n",
        "1. BOOTSTRAP SAMPLING (Bagging)\n",
        "   - Random sampling WITH replacement\n",
        "   - Each tree sees ~63% of original data\n",
        "   - Some samples appear multiple times\n",
        "   - Some samples never appear (out-of-bag samples)\n",
        "\n",
        "2. FEATURE RANDOMNESS\n",
        "   - At each split, randomly select subset of features\n",
        "   - Default: sqrt(total_features) for classification\n",
        "   - Default: total_features/3 for regression\n",
        "   - Prevents trees from being too similar\n",
        "\n",
        "3. VOTING MECHANISM\n",
        "   - Classification: Majority vote (most common prediction)\n",
        "   - Regression: Average of all predictions\n",
        "\n",
        "WHY RANDOM FOREST WORKS:\n",
        "\n",
        "‚úÖ REDUCES OVERFITTING\n",
        "   - Single tree: Memorizes training data\n",
        "   - Many trees: Errors cancel out\n",
        "\n",
        "‚úÖ HANDLES MISSING DATA\n",
        "   - Can work with incomplete features\n",
        "\n",
        "‚úÖ FEATURE IMPORTANCE\n",
        "   - Can tell which features matter most\n",
        "\n",
        "‚úÖ ROBUST TO OUTLIERS\n",
        "   - Multiple trees average out extreme predictions\n",
        "\n",
        "EXAMPLE: Predicting House Price\n",
        "  Tree 1: Focuses on size ‚Üí Predicts $300K\n",
        "  Tree 2: Focuses on location ‚Üí Predicts $350K\n",
        "  Tree 3: Focuses on age ‚Üí Predicts $280K\n",
        "  Tree 4: Focuses on bedrooms ‚Üí Predicts $320K\n",
        "  ... (100 trees)\n",
        "  \n",
        "  Final: Average = $310K (more accurate than any single tree)\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONCEPT 3: N_ESTIMATORS - NUMBER OF TREES\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CONCEPT 3: N_ESTIMATORS - NUMBER OF TREES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "üî¢ WHAT IS N_ESTIMATORS?\n",
        "\n",
        "n_estimators = Number of trees in the forest/ensemble\n",
        "\n",
        "Random Forest: n_estimators = 100 means 100 decision trees\n",
        "XGBoost: n_estimators = 100 means 100 boosting rounds (trees)\n",
        "\n",
        "HOW IT AFFECTS PERFORMANCE:\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ n_estimators‚îÇ Accuracy      ‚îÇ Training Time‚îÇ Overfitting   ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ 10          ‚îÇ Lower         ‚îÇ Fast         ‚îÇ Less risk    ‚îÇ\n",
        "‚îÇ 50          ‚îÇ Medium        ‚îÇ Medium       ‚îÇ Medium risk  ‚îÇ\n",
        "‚îÇ 100         ‚îÇ Good          ‚îÇ Slower       ‚îÇ Low risk     ‚îÇ\n",
        "‚îÇ 500         ‚îÇ Better        ‚îÇ Slow         ‚îÇ Very low risk‚îÇ\n",
        "‚îÇ 1000        ‚îÇ Best (often) ‚îÇ Very slow    ‚îÇ Minimal risk ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "TRADE-OFFS:\n",
        "\n",
        "1. MORE TREES (Higher n_estimators)\n",
        "   ‚úÖ Better accuracy (usually)\n",
        "   ‚úÖ More stable predictions\n",
        "   ‚úÖ Less overfitting\n",
        "   ‚ùå Longer training time\n",
        "   ‚ùå More memory usage\n",
        "   ‚ùå Diminishing returns (1000 trees not always much better than 500)\n",
        "\n",
        "2. FEWER TREES (Lower n_estimators)\n",
        "   ‚úÖ Faster training\n",
        "   ‚úÖ Less memory\n",
        "   ‚úÖ Quicker predictions\n",
        "   ‚ùå May be less accurate\n",
        "   ‚ùå More variance in predictions\n",
        "   ‚ùå Higher risk of underfitting\n",
        "\n",
        "DIMINISHING RETURNS:\n",
        "\n",
        "Example: Accuracy vs n_estimators\n",
        "  n_estimators=10:  75% accuracy\n",
        "  n_estimators=50:   82% accuracy  (+7%)\n",
        "  n_estimators=100:  85% accuracy  (+3%)\n",
        "  n_estimators=200:  86% accuracy  (+1%)\n",
        "  n_estimators=500:  86.5% accuracy (+0.5%)\n",
        "  n_estimators=1000: 86.6% accuracy (+0.1%)\n",
        "\n",
        "  ‚Üí After 200 trees, adding more trees doesn't help much!\n",
        "\n",
        "HOW TO CHOOSE N_ESTIMATORS:\n",
        "\n",
        "1. START WITH DEFAULT (100)\n",
        "   - Good balance of accuracy and speed\n",
        "   - Works well for most problems\n",
        "\n",
        "2. IF ACCURACY IS LOW\n",
        "   - Try increasing: 200, 500, 1000\n",
        "   - Monitor if accuracy improves\n",
        "   - Stop when improvement is minimal\n",
        "\n",
        "3. IF TRAINING IS TOO SLOW\n",
        "   - Try decreasing: 50, 25, 10\n",
        "   - Check if accuracy drops significantly\n",
        "   - Find sweet spot between speed and accuracy\n",
        "\n",
        "4. USE CROSS-VALIDATION\n",
        "   - Test different values: [10, 50, 100, 200, 500]\n",
        "   - Choose value with best validation score\n",
        "   - Consider training time in decision\n",
        "\n",
        "PRACTICAL EXAMPLE:\n",
        "\n",
        "Random Forest with different n_estimators:\n",
        "  n_estimators=10:  Training time: 2 seconds,  Accuracy: 78%\n",
        "  n_estimators=100: Training time: 20 seconds, Accuracy: 85%\n",
        "  n_estimators=500: Training time: 100 seconds, Accuracy: 86%\n",
        "  \n",
        "  ‚Üí n_estimators=100 is best choice (good accuracy, reasonable time)\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONCEPT 4: LEARNING RATE - STEP SIZE IN BOOSTING\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CONCEPT 4: LEARNING RATE - STEP SIZE IN BOOSTING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "üìà WHAT IS LEARNING RATE?\n",
        "\n",
        "Learning Rate = How much each tree contributes to the final prediction\n",
        "\n",
        "Analogy: Learning to ride a bike\n",
        "  - High learning rate: Big corrections ‚Üí Might overshoot, fall down\n",
        "  - Low learning rate: Small corrections ‚Üí Takes longer, but smoother\n",
        "  - Optimal learning rate: Just right ‚Üí Learn efficiently without falling\n",
        "\n",
        "HOW IT WORKS IN BOOSTING (XGBoost, Gradient Boosting):\n",
        "\n",
        "Sequential Learning Process:\n",
        "  Step 1: Train Tree 1 ‚Üí Makes prediction\n",
        "  Step 2: Calculate errors (what Tree 1 got wrong)\n",
        "  Step 3: Train Tree 2 to fix Tree 1's errors\n",
        "  Step 4: Combine: Final = Tree1 + (learning_rate √ó Tree2)\n",
        "  Step 5: Calculate remaining errors\n",
        "  Step 6: Train Tree 3 to fix remaining errors\n",
        "  Step 7: Combine: Final = Tree1 + (lr √ó Tree2) + (lr √ó Tree3)\n",
        "  ... (repeat for n_estimators trees)\n",
        "\n",
        "MATHEMATICAL VIEW:\n",
        "\n",
        "Final Prediction = Tree1 + (lr √ó Tree2) + (lr √ó Tree3) + ... + (lr √ó TreeN)\n",
        "\n",
        "Example with learning_rate = 0.1:\n",
        "  Tree 1 prediction: 100\n",
        "  Tree 2 correction: +20 (fixes some errors)\n",
        "  Tree 3 correction: +15 (fixes more errors)\n",
        "  Tree 4 correction: +10 (fixes remaining errors)\n",
        "  \n",
        "  Final = 100 + (0.1 √ó 20) + (0.1 √ó 15) + (0.1 √ó 10)\n",
        "        = 100 + 2 + 1.5 + 1\n",
        "        = 104.5\n",
        "\n",
        "Example with learning_rate = 0.5 (higher):\n",
        "  Final = 100 + (0.5 √ó 20) + (0.5 √ó 15) + (0.5 √ó 10)\n",
        "        = 100 + 10 + 7.5 + 5\n",
        "        = 122.5  ‚Üê Bigger steps, might overshoot!\n",
        "\n",
        "LEARNING RATE VALUES:\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Learning Rate‚îÇ Step Size    ‚îÇ Training     ‚îÇ Overfitting  ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ 0.01         ‚îÇ Very small   ‚îÇ Slow         ‚îÇ Low risk     ‚îÇ\n",
        "‚îÇ 0.1          ‚îÇ Small        ‚îÇ Medium       ‚îÇ Low risk     ‚îÇ\n",
        "‚îÇ 0.3          ‚îÇ Medium       ‚îÇ Fast         ‚îÇ Medium risk  ‚îÇ\n",
        "‚îÇ 0.5          ‚îÇ Large        ‚îÇ Very fast    ‚îÇ High risk    ‚îÇ\n",
        "‚îÇ 1.0          ‚îÇ Very large   ‚îÇ Fastest      ‚îÇ Very high    ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "TRADE-OFFS:\n",
        "\n",
        "1. HIGH LEARNING RATE (0.3 - 1.0)\n",
        "   ‚úÖ Faster convergence (fewer trees needed)\n",
        "   ‚úÖ Quicker training\n",
        "   ‚ùå May overshoot optimal solution\n",
        "   ‚ùå Higher risk of overfitting\n",
        "   ‚ùå Less stable training\n",
        "\n",
        "2. LOW LEARNING RATE (0.01 - 0.1)\n",
        "   ‚úÖ More stable training\n",
        "   ‚úÖ Better final accuracy (usually)\n",
        "   ‚úÖ Lower risk of overfitting\n",
        "   ‚ùå Slower convergence (needs more trees)\n",
        "   ‚ùå Longer training time\n",
        "\n",
        "LEARNING RATE vs N_ESTIMATORS:\n",
        "\n",
        "These two hyperparameters work together!\n",
        "\n",
        "Example 1: High LR, Few Trees\n",
        "  learning_rate = 0.5, n_estimators = 50\n",
        "  ‚Üí Fast training, but may overfit\n",
        "\n",
        "Example 2: Low LR, Many Trees\n",
        "  learning_rate = 0.01, n_estimators = 1000\n",
        "  ‚Üí Slow training, but very accurate\n",
        "\n",
        "Example 3: Balanced (Recommended)\n",
        "  learning_rate = 0.1, n_estimators = 100\n",
        "  ‚Üí Good balance of speed and accuracy\n",
        "\n",
        "RULE OF THUMB:\n",
        "  - If you increase learning_rate ‚Üí You can decrease n_estimators\n",
        "  - If you decrease learning_rate ‚Üí You should increase n_estimators\n",
        "  - Common combination: lr=0.1, n_estimators=100\n",
        "\n",
        "VISUALIZATION:\n",
        "\n",
        "Learning Rate = 0.1 (Small Steps):\n",
        "  Start: 50% accuracy\n",
        "  Tree 1: 55% accuracy (+5%)\n",
        "  Tree 2: 58% accuracy (+3%)\n",
        "  Tree 3: 60% accuracy (+2%)\n",
        "  Tree 4: 61% accuracy (+1%)\n",
        "  ... (gradual improvement, smooth curve)\n",
        "\n",
        "Learning Rate = 0.5 (Big Steps):\n",
        "  Start: 50% accuracy\n",
        "  Tree 1: 65% accuracy (+15%)\n",
        "  Tree 2: 72% accuracy (+7%)\n",
        "  Tree 3: 75% accuracy (+3%)\n",
        "  Tree 4: 76% accuracy (+1%)\n",
        "  ... (fast improvement, but may overshoot)\n",
        "\n",
        "HOW TO CHOOSE LEARNING RATE:\n",
        "\n",
        "1. START WITH DEFAULT (0.1)\n",
        "   - Works well for most problems\n",
        "   - Good starting point\n",
        "\n",
        "2. IF TRAINING IS TOO SLOW\n",
        "   - Try increasing: 0.2, 0.3\n",
        "   - Monitor for overfitting\n",
        "   - May need to reduce n_estimators\n",
        "\n",
        "3. IF OVERFITTING OCCURS\n",
        "   - Try decreasing: 0.05, 0.01\n",
        "   - May need to increase n_estimators\n",
        "   - Usually improves generalization\n",
        "\n",
        "4. USE GRID SEARCH\n",
        "   - Test combinations: [0.01, 0.1, 0.3] √ó [50, 100, 200]\n",
        "   - Find best combination\n",
        "   - Consider both accuracy and training time\n",
        "\n",
        "PRACTICAL EXAMPLE:\n",
        "\n",
        "XGBoost with different learning rates:\n",
        "  lr=0.01, n_estimators=1000: Accuracy: 88%, Time: 300s\n",
        "  lr=0.1,  n_estimators=100:  Accuracy: 87%, Time: 30s\n",
        "  lr=0.5,  n_estimators=50:   Accuracy: 85%, Time: 15s\n",
        "  \n",
        "  ‚Üí lr=0.1, n_estimators=100 is best (good accuracy, reasonable time)\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONCEPT 5: MAX_DEPTH - TREE COMPLEXITY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CONCEPT 5: MAX_DEPTH - TREE COMPLEXITY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "üå≥ WHAT IS MAX_DEPTH?\n",
        "\n",
        "max_depth = Maximum number of levels (depth) a tree can have\n",
        "\n",
        "Example: max_depth = 3\n",
        "  Level 0 (Root):        [Is age > 30?]\n",
        "                           /         \\\\\n",
        "  Level 1:          [YES]           [NO]\n",
        "                      /  \\\\            /  \\\\\n",
        "  Level 2:      [Income] [Buy]   [Income] [Don't]\n",
        "                 /  \\\\    (leaf)   /  \\\\     (leaf)\n",
        "  Level 3:   [Buy] [Don't]    [Buy] [Don't]\n",
        "            (leaf) (leaf)    (leaf) (leaf)\n",
        "\n",
        "HOW IT AFFECTS PERFORMANCE:\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ max_depth ‚îÇ Complexity   ‚îÇ Overfitting  ‚îÇ Training    ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ 1         ‚îÇ Very simple  ‚îÇ Low risk     ‚îÇ Fast        ‚îÇ\n",
        "‚îÇ 3         ‚îÇ Simple       ‚îÇ Low risk     ‚îÇ Fast        ‚îÇ\n",
        "‚îÇ 5         ‚îÇ Medium       ‚îÇ Medium risk  ‚îÇ Medium      ‚îÇ\n",
        "‚îÇ 10        ‚îÇ Complex      ‚îÇ High risk    ‚îÇ Slower      ‚îÇ\n",
        "‚îÇ None      ‚îÇ Very complex  ‚îÇ Very high    ‚îÇ Slow        ‚îÇ\n",
        "‚îÇ (unlimited)‚îÇ             ‚îÇ              ‚îÇ             ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "TRADE-OFFS:\n",
        "\n",
        "1. DEEP TREES (High max_depth)\n",
        "   ‚úÖ Can capture complex patterns\n",
        "   ‚úÖ More detailed decisions\n",
        "   ‚ùå Higher risk of overfitting\n",
        "   ‚ùå Longer training time\n",
        "   ‚ùå Harder to interpret\n",
        "\n",
        "2. SHALLOW TREES (Low max_depth)\n",
        "   ‚úÖ Faster training\n",
        "   ‚úÖ Less overfitting\n",
        "   ‚úÖ Easier to interpret\n",
        "   ‚ùå May miss complex patterns\n",
        "   ‚ùå May underfit (too simple)\n",
        "\n",
        "DEFAULT VALUES:\n",
        "  - Random Forest: Usually None (unlimited) or very high\n",
        "  - XGBoost: Usually 6 or None\n",
        "  - Reason: Ensemble methods can handle deeper trees better\n",
        "\n",
        "PRACTICAL EXAMPLE:\n",
        "\n",
        "Random Forest with different max_depth:\n",
        "  max_depth=3:  Accuracy: 80%, Training: 5s,  Overfitting: Low\n",
        "  max_depth=5:  Accuracy: 85%, Training: 8s,  Overfitting: Medium\n",
        "  max_depth=10: Accuracy: 86%, Training: 15s, Overfitting: High\n",
        "  max_depth=None: Accuracy: 87%, Training: 20s, Overfitting: Very High\n",
        "  \n",
        "  ‚Üí max_depth=5 or 10 is often best (good accuracy, manageable overfitting)\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONCEPT 6: RELATIONSHIP BETWEEN HYPERPARAMETERS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CONCEPT 6: RELATIONSHIP BETWEEN HYPERPARAMETERS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "üîó HOW HYPERPARAMETERS WORK TOGETHER:\n",
        "\n",
        "1. N_ESTIMATORS + LEARNING RATE\n",
        "   - More trees can compensate for lower learning rate\n",
        "   - Fewer trees needed with higher learning rate\n",
        "   - Balance: lr=0.1, n_estimators=100 (common default)\n",
        "\n",
        "2. MAX_DEPTH + N_ESTIMATORS\n",
        "   - Deeper trees: May need fewer trees\n",
        "   - Shallower trees: May need more trees\n",
        "   - Reason: Each tree's complexity affects how many you need\n",
        "\n",
        "3. LEARNING RATE + MAX_DEPTH\n",
        "   - High LR + Deep trees: Very fast, but risky\n",
        "   - Low LR + Shallow trees: Slow but safe\n",
        "   - Balanced: Medium LR + Medium depth\n",
        "\n",
        "HYPERPARAMETER TUNING STRATEGY:\n",
        "\n",
        "Step 1: Start with defaults\n",
        "  - n_estimators = 100\n",
        "  - learning_rate = 0.1\n",
        "  - max_depth = 6 (or None)\n",
        "\n",
        "Step 2: If accuracy is low\n",
        "  - Increase n_estimators first (200, 500)\n",
        "  - Then adjust learning_rate if needed\n",
        "  - Finally tune max_depth\n",
        "\n",
        "Step 3: If overfitting occurs\n",
        "  - Decrease max_depth first\n",
        "  - Decrease learning_rate\n",
        "  - Increase n_estimators (to compensate)\n",
        "\n",
        "Step 4: Use Grid Search or Random Search\n",
        "  - Test multiple combinations\n",
        "  - Use cross-validation\n",
        "  - Choose best based on validation score\n",
        "\n",
        "COMMON COMBINATIONS:\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Use Case     ‚îÇ n_estimators ‚îÇ learning_rate‚îÇ max_depth    ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Quick test   ‚îÇ 50           ‚îÇ 0.3          ‚îÇ 5            ‚îÇ\n",
        "‚îÇ Default      ‚îÇ 100          ‚îÇ 0.1         ‚îÇ 6            ‚îÇ\n",
        "‚îÇ High accuracy‚îÇ 500          ‚îÇ 0.05         ‚îÇ 8            ‚îÇ\n",
        "‚îÇ Fast training‚îÇ 50           ‚îÇ 0.2          ‚îÇ 4            ‚îÇ\n",
        "‚îÇ Avoid overfit‚îÇ 200          ‚îÇ 0.05         ‚îÇ 4            ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUMMARY: KEY CONCEPTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "‚úÖ DECISION TREE: Single model that asks questions to make predictions\n",
        "‚úÖ RANDOM FOREST: Many trees voting together (bagging)\n",
        "‚úÖ N_ESTIMATORS: Number of trees (more = better but slower)\n",
        "‚úÖ LEARNING RATE: Step size in boosting (lower = more stable)\n",
        "‚úÖ MAX_DEPTH: Tree complexity (deeper = more complex but risky)\n",
        "\n",
        "KEY TAKEAWAYS:\n",
        "1. More trees usually = better accuracy (but diminishing returns)\n",
        "2. Lower learning rate = more stable (but needs more trees)\n",
        "3. Deeper trees = more complex (but higher overfitting risk)\n",
        "4. Hyperparameters work together - tune them together\n",
        "5. Use cross-validation to find best combinations\n",
        "\n",
        "NEXT STEPS:\n",
        "- Experiment with different hyperparameter values\n",
        "- Use GridSearchCV for systematic tuning\n",
        "- Monitor training time vs accuracy trade-offs\n",
        "- Understand your data to choose appropriate values\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"HYPERPARAMETER GUIDE COMPLETE!\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 3: IMPLEMENTING ENSEMBLE METHODS\n",
            "======================================================================\n",
            "\n",
            "üîß Preparing features for machine learning...\n",
            "   - Dropped datetime columns: ['date']\n",
            "   - Dropped text columns: ['description']\n",
            "   - Converted categorical 'cluster_id' to int\n",
            "\n",
            "‚úÖ Clean features shape: (10000, 5)\n",
            "   - Features: ['amount', 'amount_lag1', 'amount_lag2', 'amount_rolling_mean_3', 'cluster_id']\n",
            "   - Data types: {dtype('float64'): 4, dtype('int64'): 1}\n",
            "   - All columns are numeric: True\n",
            "\n",
            "‚úÇÔ∏è  Splitting data into training and testing sets...\n",
            "‚úÖ Training set: 8000 samples\n",
            "‚úÖ Test set: 2000 samples\n",
            "\n",
            "üî¢ Encoding target variable (if needed)...\n",
            "   - Target is categorical (string labels)\n",
            "   - Encoding to numeric labels for models...\n",
            "   - Encoded 6 classes:\n",
            "     'Dining Out' ‚Üí 0\n",
            "     'Entertainment' ‚Üí 1\n",
            "     'Groceries' ‚Üí 2\n",
            "     'Healthcare' ‚Üí 3\n",
            "     'Transport' ‚Üí 4\n",
            "     'Utilities' ‚Üí 5\n",
            "‚úÖ Target variable prepared\n",
            "\n",
            "======================================================================\n",
            "ENSEMBLE 1: RANDOM FOREST (BAGGING)\n",
            "======================================================================\n",
            "\n",
            "üå≤ Training Random Forest (Bagging ensemble)...\n",
            "   - Algorithm: Bootstrap Aggregating\n",
            "   - Base models: Decision Trees\n",
            "   - Number of trees: 100\n",
            "‚úÖ Random Forest trained!\n",
            "   - Test Accuracy: 0.2500 (25.00%)\n",
            "   - Number of trees: 100\n",
            "\n",
            "======================================================================\n",
            "ENSEMBLE 2: XGBOOST (BOOSTING)\n",
            "======================================================================\n",
            "\n",
            "üöÄ Training XGBoost (Boosting ensemble)...\n",
            "   - Algorithm: Gradient Boosting\n",
            "   - Training: Sequential (one tree after another)\n",
            "   - Number of trees: 100\n",
            "‚úÖ XGBoost trained!\n",
            "   - Test Accuracy: 0.2460 (24.60%)\n",
            "   - Number of boosting rounds: 100\n",
            "\n",
            "======================================================================\n",
            "ENSEMBLE 3: HYPERPARAMETER TUNING (GRID SEARCH)\n",
            "======================================================================\n",
            "\n",
            "üîç Tuning XGBoost hyperparameters with Grid Search...\n",
            "   - Testing different parameter combinations\n",
            "   - Using 5-fold cross-validation\n",
            "   - Parameter grid: {'n_estimators': [50, 100], 'max_depth': [3, 5], 'learning_rate': [0.01, 0.1]}\n",
            "   - Total combinations: 8\n",
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "\n",
            "‚úÖ Grid Search complete!\n",
            "   - Best parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}\n",
            "   - Best CV score: 0.1932\n",
            "   - Test Accuracy: 0.2460 (24.60%)\n",
            "\n",
            "======================================================================\n",
            "COMPARISON: BAGGING vs BOOSTING\n",
            "======================================================================\n",
            "\n",
            "üìä Model Performance Comparison:\n",
            "   - Random Forest (Bagging):  0.2500 (25.00%)\n",
            "   - XGBoost (Boosting):        0.2460 (24.60%)\n",
            "   - XGBoost (Tuned):           0.2460 (24.60%)\n",
            "\n",
            "üèÜ Best Model: Random Forest with 0.2500 accuracy\n",
            "\n",
            "üí° Key Takeaways:\n",
            "   - Bagging (Random Forest): Fast, parallel training, reduces variance\n",
            "   - Boosting (XGBoost): Sequential training, reduces bias, often more accurate\n",
            "   - Hyperparameter tuning: Can improve performance significantly\n",
            "   - Different methods work better for different datasets\n",
            "\n",
            "======================================================================\n",
            "DECODING PREDICTIONS TO ORIGINAL LABELS\n",
            "======================================================================\n",
            "\n",
            "üìä Classification Report (Random Forest):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   Dining Out       0.17      0.07      0.10       338\n",
            "Entertainment       0.23      0.11      0.15       328\n",
            "    Groceries       0.28      0.16      0.21       337\n",
            "   Healthcare       0.21      0.09      0.13       331\n",
            "    Transport       0.16      0.09      0.12       330\n",
            "    Utilities       0.28      0.96      0.43       336\n",
            "\n",
            "     accuracy                           0.25      2000\n",
            "    macro avg       0.22      0.25      0.19      2000\n",
            " weighted avg       0.22      0.25      0.19      2000\n",
            "\n",
            "\n",
            "üí° Note: Predictions can be decoded using:\n",
            "   label_encoder.inverse_transform(numeric_predictions)\n",
            "   Example: label_encoder.inverse_transform([0, 1, 2])\n",
            "   ‚Üí ['Dining Out', 'Entertainment', 'Groceries']\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 3: IMPLEMENTING ENSEMBLE METHODS\n",
        "# ============================================================================\n",
        "# Now we'll implement different ensemble methods to see them in action\n",
        "# ============================================================================\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STEP 3: IMPLEMENTING ENSEMBLE METHODS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============================================================================\n",
        "# CONCEPT: PREPARING FEATURES FOR ML MODELS\n",
        "# ============================================================================\n",
        "# ML models need numeric features only. We need to:\n",
        "# 1. Drop datetime columns (or convert to numeric)\n",
        "# 2. Drop text columns (or use text preprocessing - skipped for simplicity)\n",
        "# 3. Handle categorical columns (encode if needed)\n",
        "# 4. Keep only numeric features\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nüîß Preparing features for machine learning...\")\n",
        "\n",
        "# Create a clean copy of X for modeling\n",
        "X_clean = X.copy()\n",
        "\n",
        "# Drop datetime columns (can't be used directly by models)\n",
        "datetime_cols = X_clean.select_dtypes(include=['datetime64', 'datetime']).columns\n",
        "if len(datetime_cols) > 0:\n",
        "    X_clean = X_clean.drop(columns=datetime_cols)\n",
        "    print(f\"   - Dropped datetime columns: {list(datetime_cols)}\")\n",
        "\n",
        "# Drop text columns (would need text preprocessing - skipping for simplicity)\n",
        "text_cols = X_clean.select_dtypes(include=['object']).columns\n",
        "# Keep only non-text object columns (like cluster_id if it's categorical)\n",
        "# Drop description and other text fields\n",
        "text_cols_to_drop = [col for col in text_cols if col not in ['cluster_id']]\n",
        "if len(text_cols_to_drop) > 0:\n",
        "    X_clean = X_clean.drop(columns=text_cols_to_drop)\n",
        "    print(f\"   - Dropped text columns: {text_cols_to_drop}\")\n",
        "\n",
        "# Convert categorical dtype columns to numeric (XGBoost doesn't accept categorical dtype)\n",
        "categorical_cols = X_clean.select_dtypes(include=['category']).columns\n",
        "if len(categorical_cols) > 0:\n",
        "    for col in categorical_cols:\n",
        "        # Convert categorical to numeric (int)\n",
        "        X_clean[col] = X_clean[col].astype(int)\n",
        "        print(f\"   - Converted categorical '{col}' to int\")\n",
        "\n",
        "# Convert remaining object columns to numeric if possible\n",
        "for col in X_clean.select_dtypes(include=['object']).columns:\n",
        "    try:\n",
        "        X_clean[col] = pd.to_numeric(X_clean[col], errors='coerce')\n",
        "        print(f\"   - Converted '{col}' to numeric\")\n",
        "    except:\n",
        "        # If conversion fails, encode it\n",
        "        le = LabelEncoder()\n",
        "        X_clean[col] = le.fit_transform(X_clean[col].astype(str))\n",
        "        print(f\"   - Encoded '{col}' as categorical\")\n",
        "\n",
        "# Ensure all columns are numeric (int, float, or bool) for XGBoost compatibility\n",
        "# Convert any remaining non-numeric types\n",
        "for col in X_clean.columns:\n",
        "    if X_clean[col].dtype not in ['int64', 'int32', 'float64', 'float32', 'bool']:\n",
        "        try:\n",
        "            X_clean[col] = pd.to_numeric(X_clean[col], errors='coerce')\n",
        "            print(f\"   - Converted '{col}' to numeric (final check)\")\n",
        "        except:\n",
        "            # Last resort: encode as integer\n",
        "            X_clean[col] = pd.Categorical(X_clean[col]).codes\n",
        "            print(f\"   - Encoded '{col}' as integer codes\")\n",
        "\n",
        "# Fill any remaining NaN values\n",
        "X_clean = X_clean.fillna(0)\n",
        "\n",
        "# Final verification: Ensure all columns are numeric types compatible with XGBoost\n",
        "# Convert to standard numeric types (int64, float64)\n",
        "for col in X_clean.columns:\n",
        "    if X_clean[col].dtype == 'bool':\n",
        "        X_clean[col] = X_clean[col].astype(int)\n",
        "    elif X_clean[col].dtype.name.startswith('int'):\n",
        "        X_clean[col] = X_clean[col].astype('int64')\n",
        "    elif X_clean[col].dtype.name.startswith('float'):\n",
        "        X_clean[col] = X_clean[col].astype('float64')\n",
        "\n",
        "print(f\"\\n‚úÖ Clean features shape: {X_clean.shape}\")\n",
        "print(f\"   - Features: {list(X_clean.columns)}\")\n",
        "print(f\"   - Data types: {X_clean.dtypes.value_counts().to_dict()}\")\n",
        "print(f\"   - All columns are numeric: {all(X_clean[col].dtype in ['int64', 'float64'] for col in X_clean.columns)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONCEPT: TRAIN-TEST SPLIT\n",
        "# ============================================================================\n",
        "# Split data into training and testing sets\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n‚úÇÔ∏è  Splitting data into training and testing sets...\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_clean, y, \n",
        "    test_size=0.2, \n",
        "    random_state=42,\n",
        "    stratify=y if y.dtype == 'object' else None  # Stratify for classification\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"‚úÖ Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONCEPT: ENCODING TARGET VARIABLE\n",
        "# ============================================================================\n",
        "# Some models (like XGBoost) require numeric class labels\n",
        "# - String labels: 'Dining Out', 'Entertainment', etc.\n",
        "# - Numeric labels: 0, 1, 2, 3, 4, 5\n",
        "#\n",
        "# LabelEncoder: Converts string labels to numeric labels\n",
        "# - 'Dining Out' ‚Üí 0\n",
        "# - 'Entertainment' ‚Üí 1\n",
        "# - 'Groceries' ‚Üí 2\n",
        "# - etc.\n",
        "#\n",
        "# We'll encode y for XGBoost, but keep original for RandomForest\n",
        "# (RandomForest can handle strings, but encoding is more consistent)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nüî¢ Encoding target variable (if needed)...\")\n",
        "\n",
        "# Check if y needs encoding (if it's string/object type)\n",
        "if y_train.dtype == 'object' or isinstance(y_train.iloc[0] if len(y_train) > 0 else None, str):\n",
        "    print(\"   - Target is categorical (string labels)\")\n",
        "    print(\"   - Encoding to numeric labels for models...\")\n",
        "    \n",
        "    # Create label encoder\n",
        "    label_encoder = LabelEncoder()\n",
        "    \n",
        "    # Fit on training data and transform both train and test\n",
        "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "    y_test_encoded = label_encoder.transform(y_test)\n",
        "    \n",
        "    # Store class mapping for later\n",
        "    class_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "    \n",
        "    print(f\"   - Encoded {len(class_mapping)} classes:\")\n",
        "    for class_name, class_id in class_mapping.items():\n",
        "        print(f\"     '{class_name}' ‚Üí {class_id}\")\n",
        "    \n",
        "    # Use encoded labels for training\n",
        "    y_train_numeric = y_train_encoded\n",
        "    y_test_numeric = y_test_encoded\n",
        "    y_train_original = y_train.copy()  # Keep original for display\n",
        "    y_test_original = y_test.copy()\n",
        "else:\n",
        "    print(\"   - Target is already numeric\")\n",
        "    y_train_numeric = y_train\n",
        "    y_test_numeric = y_test\n",
        "    y_train_original = y_train\n",
        "    y_test_original = y_test\n",
        "    label_encoder = None\n",
        "    class_mapping = None\n",
        "\n",
        "print(\"‚úÖ Target variable prepared\")\n",
        "\n",
        "# ============================================================================\n",
        "# ENSEMBLE 1: RANDOM FOREST (BAGGING)\n",
        "# ============================================================================\n",
        "# Random Forest = Bagging of Decision Trees\n",
        "# - Multiple trees trained on bootstrap samples\n",
        "# - Final prediction = majority vote\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ENSEMBLE 1: RANDOM FOREST (BAGGING)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüå≤ Training Random Forest (Bagging ensemble)...\")\n",
        "print(\"   - Algorithm: Bootstrap Aggregating\")\n",
        "print(\"   - Base models: Decision Trees\")\n",
        "print(\"   - Number of trees: 100\")\n",
        "\n",
        "# Basic Ensemble: RandomForest (bagging)\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,      # Number of trees (base models)\n",
        "    random_state=42,        # For reproducibility\n",
        "    max_depth=10,           # Limit tree depth (prevent overfitting)\n",
        "    n_jobs=-1               # Use all CPU cores\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train_numeric)\n",
        "\n",
        "# Evaluate\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test_numeric, rf_pred)\n",
        "\n",
        "print(f\"‚úÖ Random Forest trained!\")\n",
        "print(f\"   - Test Accuracy: {rf_accuracy:.4f} ({rf_accuracy*100:.2f}%)\")\n",
        "print(f\"   - Number of trees: {rf.n_estimators}\")\n",
        "\n",
        "# ============================================================================\n",
        "# ENSEMBLE 2: XGBOOST (BOOSTING)\n",
        "# ============================================================================\n",
        "# XGBoost = Gradient Boosting with optimizations\n",
        "# - Sequential training (each tree corrects previous errors)\n",
        "# - Very popular and powerful\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ENSEMBLE 2: XGBOOST (BOOSTING)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüöÄ Training XGBoost (Boosting ensemble)...\")\n",
        "print(\"   - Algorithm: Gradient Boosting\")\n",
        "print(\"   - Training: Sequential (one tree after another)\")\n",
        "print(\"   - Number of trees: 100\")\n",
        "\n",
        "# Advanced: XGBoost (boosting)\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=100,       # Number of boosting rounds (trees)\n",
        "    learning_rate=0.1,      # Step size (how much each tree contributes)\n",
        "    random_state=42,        # For reproducibility\n",
        "    max_depth=5,            # Maximum tree depth\n",
        "    n_jobs=-1,              # Use all CPU cores\n",
        "    eval_metric='mlogloss'  # Multi-class log loss\n",
        ")\n",
        "\n",
        "xgb.fit(X_train, y_train_numeric)\n",
        "\n",
        "# Evaluate\n",
        "xgb_pred = xgb.predict(X_test)\n",
        "xgb_accuracy = accuracy_score(y_test_numeric, xgb_pred)\n",
        "\n",
        "print(f\"‚úÖ XGBoost trained!\")\n",
        "print(f\"   - Test Accuracy: {xgb_accuracy:.4f} ({xgb_accuracy*100:.2f}%)\")\n",
        "print(f\"   - Number of boosting rounds: {xgb.n_estimators}\")\n",
        "\n",
        "# ============================================================================\n",
        "# ENSEMBLE 3: HYPERPARAMETER TUNING WITH GRID SEARCH\n",
        "# ============================================================================\n",
        "# Grid Search: Try different hyperparameter combinations\n",
        "# - Finds best parameters for the model\n",
        "# - Uses cross-validation for robust evaluation\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ENSEMBLE 3: HYPERPARAMETER TUNING (GRID SEARCH)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüîç Tuning XGBoost hyperparameters with Grid Search...\")\n",
        "print(\"   - Testing different parameter combinations\")\n",
        "print(\"   - Using 5-fold cross-validation\")\n",
        "\n",
        "# Tuning with GridSearch\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],      # Number of trees\n",
        "    'max_depth': [3, 5],           # Tree depth\n",
        "    'learning_rate': [0.01, 0.1]   # Learning rate\n",
        "}\n",
        "\n",
        "print(f\"   - Parameter grid: {param_grid}\")\n",
        "print(f\"   - Total combinations: {len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['learning_rate'])}\")\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    XGBClassifier(random_state=42, n_jobs=-1, eval_metric='mlogloss'),\n",
        "    param_grid, \n",
        "    cv=5,                          # 5-fold cross-validation\n",
        "    scoring='f1_macro',            # F1-macro score (good for multi-class)\n",
        "    n_jobs=-1,                     # Use all CPU cores\n",
        "    verbose=1                      # Show progress\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train_numeric)\n",
        "best_xgb = grid.best_estimator_\n",
        "\n",
        "print(f\"\\n‚úÖ Grid Search complete!\")\n",
        "print(f\"   - Best parameters: {grid.best_params_}\")\n",
        "print(f\"   - Best CV score: {grid.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate best model\n",
        "best_pred = best_xgb.predict(X_test)\n",
        "best_accuracy = accuracy_score(y_test_numeric, best_pred)\n",
        "\n",
        "print(f\"   - Test Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# COMPARISON: BAGGING vs BOOSTING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"COMPARISON: BAGGING vs BOOSTING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nüìä Model Performance Comparison:\")\n",
        "print(f\"   - Random Forest (Bagging):  {rf_accuracy:.4f} ({rf_accuracy*100:.2f}%)\")\n",
        "print(f\"   - XGBoost (Boosting):        {xgb_accuracy:.4f} ({xgb_accuracy*100:.2f}%)\")\n",
        "print(f\"   - XGBoost (Tuned):           {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
        "\n",
        "best_model = max([\n",
        "    ('Random Forest', rf_accuracy),\n",
        "    ('XGBoost (Default)', xgb_accuracy),\n",
        "    ('XGBoost (Tuned)', best_accuracy)\n",
        "], key=lambda x: x[1])\n",
        "\n",
        "print(f\"\\nüèÜ Best Model: {best_model[0]} with {best_model[1]:.4f} accuracy\")\n",
        "\n",
        "print(\"\\nüí° Key Takeaways:\")\n",
        "print(\"   - Bagging (Random Forest): Fast, parallel training, reduces variance\")\n",
        "print(\"   - Boosting (XGBoost): Sequential training, reduces bias, often more accurate\")\n",
        "print(\"   - Hyperparameter tuning: Can improve performance significantly\")\n",
        "print(\"   - Different methods work better for different datasets\")\n",
        "\n",
        "# ============================================================================\n",
        "# DECODING PREDICTIONS (Optional - for interpretation)\n",
        "# ============================================================================\n",
        "# If we encoded the target, we can decode predictions back to original labels\n",
        "# ============================================================================\n",
        "\n",
        "if label_encoder is not None:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"DECODING PREDICTIONS TO ORIGINAL LABELS\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Decode predictions back to original class names\n",
        "    rf_pred_decoded = label_encoder.inverse_transform(rf_pred)\n",
        "    xgb_pred_decoded = label_encoder.inverse_transform(xgb_pred)\n",
        "    best_pred_decoded = label_encoder.inverse_transform(best_pred)\n",
        "    \n",
        "    print(\"\\nüìä Classification Report (Random Forest):\")\n",
        "    print(classification_report(y_test_original, rf_pred_decoded))\n",
        "    \n",
        "    print(\"\\nüí° Note: Predictions can be decoded using:\")\n",
        "    print(\"   label_encoder.inverse_transform(numeric_predictions)\")\n",
        "    print(\"   Example: label_encoder.inverse_transform([0, 1, 2])\")\n",
        "    print(\"   ‚Üí ['Dining Out', 'Entertainment', 'Groceries']\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Predictions decoded to original class names\n",
            "\n",
            "üìä Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   Dining Out       0.17      0.08      0.11       338\n",
            "Entertainment       0.18      0.10      0.13       328\n",
            "    Groceries       0.24      0.14      0.18       337\n",
            "   Healthcare       0.26      0.12      0.17       331\n",
            "    Transport       0.18      0.11      0.13       330\n",
            "    Utilities       0.28      0.91      0.43       336\n",
            "\n",
            "     accuracy                           0.25      2000\n",
            "    macro avg       0.22      0.24      0.19      2000\n",
            " weighted avg       0.22      0.25      0.19      2000\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Feature Importance:\n",
            "                 feature  importance\n",
            "4             cluster_id    0.889516\n",
            "0                 amount    0.035893\n",
            "3  amount_rolling_mean_3    0.025097\n",
            "2            amount_lag2    0.025015\n",
            "1            amount_lag1    0.024479\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/wc/d50td_fd3y79bpzgzcdvhgp80000gn/T/ipykernel_56478/1372321456.py:39: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.barplot(x='importance', y='feature', data=feat_df, palette='viridis')\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Get predictions (numeric labels)\n",
        "y_pred_numeric = best_xgb.predict(X_test)\n",
        "\n",
        "# Decode predictions to original labels (if label_encoder exists)\n",
        "if 'label_encoder' in globals() and label_encoder is not None:\n",
        "    y_pred = label_encoder.inverse_transform(y_pred_numeric)\n",
        "    y_test_eval = y_test_original  # Use original labels for evaluation\n",
        "    print(\"‚úÖ Predictions decoded to original class names\")\n",
        "else:\n",
        "    y_pred = y_pred_numeric\n",
        "    y_test_eval = y_test_numeric  # Use numeric labels if no encoder\n",
        "    print(\"‚úÖ Using numeric labels for evaluation\")\n",
        "\n",
        "print(\"\\nüìä Classification Report:\")\n",
        "print(classification_report(y_test_eval, y_pred))\n",
        "\n",
        "# Confusion Matrix Visualization\n",
        "cm = confusion_matrix(y_test_eval, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=label_encoder.classes_ if 'label_encoder' in globals() and label_encoder is not None else None,\n",
        "            yticklabels=label_encoder.classes_ if 'label_encoder' in globals() and label_encoder is not None else None)\n",
        "plt.title('Confusion Matrix - Best XGBoost Model', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Actual Class', fontsize=12)\n",
        "plt.xlabel('Predicted Class', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Feature Importance\n",
        "importances = best_xgb.feature_importances_\n",
        "feat_df = pd.DataFrame({'feature': X_test.columns, 'importance': importances}).sort_values('importance', ascending=False)\n",
        "print(\"\\nüìä Feature Importance:\")\n",
        "print(feat_df)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='importance', y='feature', data=feat_df, palette='viridis')\n",
        "plt.title('Feature Importance - Best XGBoost Model', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Importance Score', fontsize=12)\n",
        "plt.ylabel('Feature', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 6: COMPREHENSIVE GUIDE TO EVALUATION METRICS & FEATURE IMPORTANCE\n",
        "# ============================================================================\n",
        "# Deep dive into evaluation concepts, F1-score, and feature importance\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"COMPREHENSIVE GUIDE: EVALUATION METRICS & FEATURE IMPORTANCE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============================================================================\n",
        "# CONCEPT 1: EVALUATION METRICS - WHY MULTIPLE METRICS?\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CONCEPT 1: EVALUATION METRICS - WHY MULTIPLE METRICS?\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "üìä WHY NOT JUST USE ACCURACY?\n",
        "\n",
        "Accuracy alone can be misleading, especially with imbalanced data!\n",
        "\n",
        "Example: Medical Diagnosis (Cancer Detection)\n",
        "  - 1000 patients: 990 healthy, 10 have cancer\n",
        "  - Model predicts \"healthy\" for everyone\n",
        "  - Accuracy = 990/1000 = 99% ‚úÖ\n",
        "  - But: Found 0/10 cancers ‚ùå\n",
        "  - This is a TERRIBLE model for cancer detection!\n",
        "\n",
        "‚Üí We need metrics that tell us MORE than just overall correctness\n",
        "\n",
        "KEY EVALUATION METRICS:\n",
        "\n",
        "1. ACCURACY\n",
        "   - What: Overall percentage of correct predictions\n",
        "   - Formula: (Correct Predictions) / (Total Predictions)\n",
        "   - When to use: Balanced datasets, when all classes matter equally\n",
        "   - Limitation: Misleading with imbalanced data\n",
        "\n",
        "2. PRECISION\n",
        "   - What: Of predictions for a class, how many were correct?\n",
        "   - Formula: True Positives / (True Positives + False Positives)\n",
        "   - Question: \"When I predict X, how often am I right?\"\n",
        "   - When to use: When false positives are costly\n",
        "   - Example: Spam detection (don't want to mark real emails as spam)\n",
        "\n",
        "3. RECALL (Sensitivity)\n",
        "   - What: Of actual examples in a class, how many did we find?\n",
        "   - Formula: True Positives / (True Positives + False Negatives)\n",
        "   - Question: \"Of all actual X, how many did I catch?\"\n",
        "   - When to use: When false negatives are costly\n",
        "   - Example: Cancer detection (don't want to miss any cancers)\n",
        "\n",
        "4. F1-SCORE\n",
        "   - What: Harmonic mean of Precision and Recall\n",
        "   - Formula: 2 √ó (Precision √ó Recall) / (Precision + Recall)\n",
        "   - When to use: When you need balance between precision and recall\n",
        "   - Best for: Imbalanced datasets, when both false positives and false negatives matter\n",
        "\n",
        "5. SUPPORT\n",
        "   - What: Number of actual examples in each class\n",
        "   - Purpose: Shows how many examples each metric is based on\n",
        "   - Helps: Understand if metrics are reliable (more support = more reliable)\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONCEPT 2: F1-SCORE - DEEP DIVE\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CONCEPT 2: F1-SCORE - DEEP DIVE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "üéØ WHAT IS F1-SCORE?\n",
        "\n",
        "F1-Score = Harmonic Mean of Precision and Recall\n",
        "\n",
        "Why \"Harmonic Mean\" instead of regular average?\n",
        "  - Regular average: (Precision + Recall) / 2\n",
        "  - Harmonic mean: 2 √ó (Precision √ó Recall) / (Precision + Recall)\n",
        "  - Harmonic mean penalizes extreme values more!\n",
        "\n",
        "Example Comparison:\n",
        "\n",
        "Scenario 1: Precision = 0.9, Recall = 0.1\n",
        "  Regular Average: (0.9 + 0.1) / 2 = 0.5\n",
        "  Harmonic Mean: 2 √ó (0.9 √ó 0.1) / (0.9 + 0.1) = 0.18\n",
        "  ‚Üí Harmonic mean shows the model is actually poor!\n",
        "\n",
        "Scenario 2: Precision = 0.5, Recall = 0.5\n",
        "  Regular Average: (0.5 + 0.5) / 2 = 0.5\n",
        "  Harmonic Mean: 2 √ó (0.5 √ó 0.5) / (0.5 + 0.5) = 0.5\n",
        "  ‚Üí Both are equal when balanced\n",
        "\n",
        "WHY F1-SCORE MATTERS:\n",
        "\n",
        "1. BALANCES PRECISION AND RECALL\n",
        "   - High Precision + Low Recall ‚Üí Low F1\n",
        "   - Low Precision + High Recall ‚Üí Low F1\n",
        "   - High Precision + High Recall ‚Üí High F1\n",
        "   - Forces you to optimize both!\n",
        "\n",
        "2. HANDLES IMBALANCED DATA\n",
        "   - Accuracy can be misleading with imbalanced classes\n",
        "   - F1-score gives equal weight to each class (when using macro-average)\n",
        "   - Better indicator of model quality\n",
        "\n",
        "3. SINGLE METRIC TO OPTIMIZE\n",
        "   - Instead of optimizing precision OR recall\n",
        "   - Optimize F1-score to balance both\n",
        "   - Simpler hyperparameter tuning\n",
        "\n",
        "F1-SCORE RANGES:\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ F1-Score ‚îÇ Quality      ‚îÇ Interpretation              ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ 0.9-1.0  ‚îÇ Excellent    ‚îÇ Model is very good         ‚îÇ\n",
        "‚îÇ 0.7-0.9  ‚îÇ Good         ‚îÇ Model is reliable          ‚îÇ\n",
        "‚îÇ 0.5-0.7  ‚îÇ Fair         ‚îÇ Model needs improvement    ‚îÇ\n",
        "‚îÇ 0.3-0.5  ‚îÇ Poor         ‚îÇ Model has issues           ‚îÇ\n",
        "‚îÇ 0.0-0.3  ‚îÇ Very Poor    ‚îÇ Model is not working well  ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "F1-SCORE VARIANTS:\n",
        "\n",
        "1. F1-Micro (Micro-Averaged F1)\n",
        "   - Calculates F1 globally across all classes\n",
        "   - Treats each prediction equally\n",
        "   - Good for: Overall model performance\n",
        "   - Formula: Uses total TP, FP, FN across all classes\n",
        "\n",
        "2. F1-Macro (Macro-Averaged F1)\n",
        "   - Calculates F1 for each class, then averages\n",
        "   - Treats each class equally (regardless of class size)\n",
        "   - Good for: Imbalanced datasets\n",
        "   - Formula: Average of F1-scores for each class\n",
        "\n",
        "3. F1-Weighted (Weighted-Averaged F1)\n",
        "   - Calculates F1 for each class, then weighted average\n",
        "   - Weights by number of samples in each class\n",
        "   - Good for: When class sizes matter\n",
        "   - Formula: Weighted average of F1-scores\n",
        "\n",
        "PRACTICAL EXAMPLE:\n",
        "\n",
        "Email Classification: Spam vs Not Spam\n",
        "  Class 1 (Spam): Precision=0.95, Recall=0.80\n",
        "    F1 = 2 √ó (0.95 √ó 0.80) / (0.95 + 0.80) = 0.87\n",
        "  \n",
        "  Class 2 (Not Spam): Precision=0.85, Recall=0.98\n",
        "    F1 = 2 √ó (0.85 √ó 0.98) / (0.85 + 0.98) = 0.91\n",
        "  \n",
        "  Macro F1 = (0.87 + 0.91) / 2 = 0.89\n",
        "  ‚Üí Good overall performance!\n",
        "\n",
        "WHEN TO USE F1-SCORE:\n",
        "\n",
        "‚úÖ Use F1-Score when:\n",
        "  - You have imbalanced classes\n",
        "  - Both precision and recall matter\n",
        "  - You need a single metric to optimize\n",
        "  - False positives AND false negatives are costly\n",
        "\n",
        "‚ùå Don't rely ONLY on F1-Score when:\n",
        "  - Classes are very imbalanced (use per-class F1)\n",
        "  - One metric is more important (use precision OR recall)\n",
        "  - You need to understand specific failures (use confusion matrix)\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONCEPT 3: CONFUSION MATRIX - VISUALIZING ERRORS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CONCEPT 3: CONFUSION MATRIX - VISUALIZING ERRORS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "üìã WHAT IS A CONFUSION MATRIX?\n",
        "\n",
        "A table showing how predictions compare to actual values\n",
        "\n",
        "Structure:\n",
        "                    Predicted\n",
        "                  Class A  Class B  Class C\n",
        "  Actual  Class A    TP      FP      FP\n",
        "          Class B    FN      TP      FP\n",
        "          Class C    FN      FN      TP\n",
        "\n",
        "  TP = True Positive (correct prediction)\n",
        "  FP = False Positive (predicted A, but was actually B/C)\n",
        "  FN = False Negative (was A, but predicted B/C)\n",
        "\n",
        "EXAMPLE: Expense Category Prediction\n",
        "\n",
        "                    Predicted\n",
        "              Dining  Groceries  Transport\n",
        "  Actual  Dining      150        20        10\n",
        "          Groceries    15       180        5\n",
        "          Transport    10        5        185\n",
        "\n",
        "Interpretation:\n",
        "  - Dining: 150 correct, 20 confused with Groceries, 10 with Transport\n",
        "  - Groceries: 180 correct, 15 confused with Dining, 5 with Transport\n",
        "  - Transport: 185 correct, 10 confused with Dining, 5 with Groceries\n",
        "\n",
        "WHAT IT TELLS US:\n",
        "\n",
        "1. DIAGONAL VALUES (Correct Predictions)\n",
        "   - Higher = better\n",
        "   - Shows which classes are predicted well\n",
        "\n",
        "2. OFF-DIAGONAL VALUES (Misclassifications)\n",
        "   - Shows which classes are confused with each other\n",
        "   - Helps identify similar/problematic classes\n",
        "\n",
        "3. PATTERNS\n",
        "   - If one class has many misclassifications ‚Üí Hard to predict\n",
        "   - If two classes are often confused ‚Üí They might be similar\n",
        "   - If all off-diagonal values are low ‚Üí Model is good\n",
        "\n",
        "HOW TO READ A CONFUSION MATRIX:\n",
        "\n",
        "Step 1: Look at diagonal (correct predictions)\n",
        "  - Are they high? ‚Üí Good!\n",
        "  - Are they low? ‚Üí Model struggles with that class\n",
        "\n",
        "Step 2: Look at rows (actual classes)\n",
        "  - Where do errors go? ‚Üí Which classes are confused?\n",
        "  - Are errors spread out? ‚Üí Model is uncertain\n",
        "\n",
        "Step 3: Look at columns (predicted classes)\n",
        "  - Which predictions are often wrong? ‚Üí Overconfident class?\n",
        "  - Are predictions balanced? ‚Üí Model bias?\n",
        "\n",
        "PRACTICAL USE:\n",
        "\n",
        "1. IDENTIFY PROBLEM CLASSES\n",
        "   - Low diagonal value = hard to predict class\n",
        "   - Many off-diagonal values = confused class\n",
        "\n",
        "2. FEATURE ENGINEERING\n",
        "   - If A and B are confused ‚Üí Add features to distinguish them\n",
        "   - If one class always wrong ‚Üí Check if features are missing\n",
        "\n",
        "3. MODEL IMPROVEMENT\n",
        "   - Focus on classes with low precision/recall\n",
        "   - Collect more data for confused classes\n",
        "   - Adjust class weights for imbalanced classes\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONCEPT 4: PRECISION, RECALL, AND THEIR TRADE-OFF\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CONCEPT 4: PRECISION, RECALL, AND THEIR TRADE-OFF\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "‚öñÔ∏è THE PRECISION-RECALL TRADE-OFF\n",
        "\n",
        "Precision and Recall often work against each other!\n",
        "\n",
        "Example: Spam Detection\n",
        "\n",
        "High Precision (Low Recall):\n",
        "  - Only predict \"spam\" when 100% sure\n",
        "  - Result: Few false positives (good!)\n",
        "  - But: Miss many actual spam emails (bad!)\n",
        "  - Precision: 0.95, Recall: 0.30\n",
        "\n",
        "High Recall (Low Precision):\n",
        "  - Predict \"spam\" when even slightly suspicious\n",
        "  - Result: Catch most spam emails (good!)\n",
        "  - But: Mark many real emails as spam (bad!)\n",
        "  - Precision: 0.60, Recall: 0.95\n",
        "\n",
        "Balanced (Good F1-Score):\n",
        "  - Find middle ground\n",
        "  - Result: Reasonable precision and recall\n",
        "  - Precision: 0.85, Recall: 0.80, F1: 0.82\n",
        "\n",
        "WHEN TO PRIORITIZE PRECISION:\n",
        "\n",
        "‚úÖ High Precision Important:\n",
        "  - Email spam detection (don't block important emails)\n",
        "  - Medical diagnosis (don't give false positives)\n",
        "  - Fraud detection (don't block legitimate transactions)\n",
        "  - Recommendation systems (only show relevant items)\n",
        "\n",
        "Example: \"Only recommend products I'm 95% sure they'll like\"\n",
        "\n",
        "WHEN TO PRIORITIZE RECALL:\n",
        "\n",
        "‚úÖ High Recall Important:\n",
        "  - Cancer detection (don't miss any cancers)\n",
        "  - Security systems (don't miss threats)\n",
        "  - Search engines (don't miss relevant results)\n",
        "  - Quality control (don't miss defective products)\n",
        "\n",
        "Example: \"Catch 99% of defective products, even if some false alarms\"\n",
        "\n",
        "WHEN TO BALANCE (F1-SCORE):\n",
        "\n",
        "‚úÖ Balance Important:\n",
        "  - General classification tasks\n",
        "  - When both errors are costly\n",
        "  - When you need reliable predictions\n",
        "  - When optimizing for overall performance\n",
        "\n",
        "Example: \"Balance between catching spam and not blocking real emails\"\n",
        "\n",
        "PRECISION-RECALL CURVE:\n",
        "\n",
        "Visual representation of the trade-off:\n",
        "\n",
        "Recall (x-axis) vs Precision (y-axis)\n",
        "\n",
        "  Precision\n",
        "    1.0 |‚óè\n",
        "        |  ‚óè\n",
        "    0.8 |    ‚óè\n",
        "        |      ‚óè\n",
        "    0.6 |        ‚óè\n",
        "        |          ‚óè\n",
        "    0.4 |            ‚óè\n",
        "        |              ‚óè\n",
        "    0.2 |                ‚óè\n",
        "        |                  ‚óè\n",
        "    0.0 |____________________‚óè\n",
        "        0.0  0.2  0.4  0.6  0.8  1.0  Recall\n",
        "\n",
        "  - Higher curve = Better model\n",
        "  - Area under curve (AUC-PR) = Overall performance metric\n",
        "  - Good for: Imbalanced datasets\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONCEPT 5: FEATURE IMPORTANCE - WHAT MATTERS MOST?\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CONCEPT 5: FEATURE IMPORTANCE - WHAT MATTERS MOST?\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "üîç WHAT IS FEATURE IMPORTANCE?\n",
        "\n",
        "Feature Importance = How much each feature contributes to predictions\n",
        "\n",
        "Think of it as: \"Which features does the model rely on most?\"\n",
        "\n",
        "EXAMPLE: Predicting House Price\n",
        "\n",
        "Features:\n",
        "  - Size (sq ft)\n",
        "  - Location (zip code)\n",
        "  - Age (years)\n",
        "  - Bedrooms\n",
        "  - Bathrooms\n",
        "\n",
        "Feature Importance:\n",
        "  - Location: 0.45 (45% importance)\n",
        "  - Size: 0.30 (30% importance)\n",
        "  - Age: 0.15 (15% importance)\n",
        "  - Bedrooms: 0.07 (7% importance)\n",
        "  - Bathrooms: 0.03 (3% importance)\n",
        "\n",
        "Interpretation:\n",
        "  - Location matters most (45%)\n",
        "  - Size is second most important (30%)\n",
        "  - Bedrooms and bathrooms matter less (7% + 3%)\n",
        "\n",
        "HOW FEATURE IMPORTANCE IS CALCULATED:\n",
        "\n",
        "1. TREE-BASED MODELS (Random Forest, XGBoost)\n",
        "   - Count how often feature is used for splits\n",
        "   - Weight by how much it improves predictions\n",
        "   - Average across all trees\n",
        "   - Higher usage = Higher importance\n",
        "\n",
        "2. LINEAR MODELS (Logistic Regression, Linear Regression)\n",
        "   - Use coefficient magnitudes\n",
        "   - Larger absolute coefficient = More important\n",
        "   - Normalize by feature scale\n",
        "\n",
        "3. PERMUTATION IMPORTANCE\n",
        "   - Shuffle one feature at a time\n",
        "   - See how much accuracy drops\n",
        "   - Larger drop = More important feature\n",
        "\n",
        "WHAT FEATURE IMPORTANCE TELLS US:\n",
        "\n",
        "1. WHICH FEATURES MATTER\n",
        "   - High importance = Feature is useful\n",
        "   - Low importance = Feature might be redundant\n",
        "   - Zero importance = Feature is not used\n",
        "\n",
        "2. FEATURE SELECTION\n",
        "   - Remove low-importance features\n",
        "   - Keep only important features\n",
        "   - Reduces model complexity\n",
        "   - May improve performance\n",
        "\n",
        "3. DOMAIN INSIGHTS\n",
        "   - Understand what drives predictions\n",
        "   - Validate business assumptions\n",
        "   - Discover unexpected patterns\n",
        "\n",
        "4. MODEL INTERPRETABILITY\n",
        "   - Explain why model makes predictions\n",
        "   - Build trust with stakeholders\n",
        "   - Debug model behavior\n",
        "\n",
        "FEATURE IMPORTANCE INTERPRETATION:\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Importance   ‚îÇ Value Range  ‚îÇ Interpretation              ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Very High     ‚îÇ 0.3 - 1.0    ‚îÇ Critical feature           ‚îÇ\n",
        "‚îÇ High         ‚îÇ 0.1 - 0.3    ‚îÇ Important feature          ‚îÇ\n",
        "‚îÇ Medium       ‚îÇ 0.05 - 0.1   ‚îÇ Useful feature             ‚îÇ\n",
        "‚îÇ Low          ‚îÇ 0.01 - 0.05  ‚îÇ Minor feature              ‚îÇ\n",
        "‚îÇ Very Low     ‚îÇ 0.0 - 0.01   ‚îÇ Negligible/Redundant       ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "CAUTIONS WITH FEATURE IMPORTANCE:\n",
        "\n",
        "1. CORRELATED FEATURES\n",
        "   - If two features are highly correlated\n",
        "   - Importance might be split between them\n",
        "   - One might show low importance even if it's useful\n",
        "   - Solution: Check feature correlations\n",
        "\n",
        "2. SCALE DEPENDENCE\n",
        "   - Some methods are sensitive to feature scale\n",
        "   - Normalize features before calculating importance\n",
        "   - Use permutation importance for scale-independent measure\n",
        "\n",
        "3. NOT CAUSAL\n",
        "   - High importance ‚â† Feature causes the outcome\n",
        "   - Might be correlation, not causation\n",
        "   - Example: \"Ice cream sales\" and \"Swimming pool accidents\"\n",
        "   - Both correlate with \"Summer\" (actual cause)\n",
        "\n",
        "4. MODEL-SPECIFIC\n",
        "   - Different models may show different importances\n",
        "   - Random Forest vs XGBoost might differ\n",
        "   - Use multiple models to validate\n",
        "\n",
        "PRACTICAL APPLICATIONS:\n",
        "\n",
        "1. FEATURE ENGINEERING\n",
        "   - Focus on high-importance features\n",
        "   - Create new features from important ones\n",
        "   - Remove or combine low-importance features\n",
        "\n",
        "2. DATA COLLECTION\n",
        "   - Prioritize collecting high-importance features\n",
        "   - May not need to collect low-importance features\n",
        "   - Saves time and resources\n",
        "\n",
        "3. MODEL SIMPLIFICATION\n",
        "   - Remove features with < 1% importance\n",
        "   - Reduces overfitting risk\n",
        "   - Faster training and prediction\n",
        "\n",
        "4. BUSINESS INSIGHTS\n",
        "   - Understand what drives business outcomes\n",
        "   - Validate hypotheses\n",
        "   - Guide strategic decisions\n",
        "\n",
        "EXAMPLE: Expense Category Prediction\n",
        "\n",
        "Feature Importance Results:\n",
        "  - cluster_id: 0.89 (89%) ‚Üí Very important!\n",
        "  - amount: 0.04 (4%) ‚Üí Minor importance\n",
        "  - amount_lag1: 0.03 (3%) ‚Üí Minor importance\n",
        "  - amount_lag2: 0.02 (2%) ‚Üí Minor importance\n",
        "  - amount_rolling_mean_3: 0.02 (2%) ‚Üí Minor importance\n",
        "\n",
        "Interpretation:\n",
        "  - User cluster is the most important predictor\n",
        "  - Spending amount and history matter less\n",
        "  - Suggests: User behavior patterns (clusters) drive categories\n",
        "  - Action: Focus on improving cluster features\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONCEPT 6: PUTTING IT ALL TOGETHER\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CONCEPT 6: PUTTING IT ALL TOGETHER\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "üîó HOW EVALUATION METRICS AND FEATURE IMPORTANCE WORK TOGETHER:\n",
        "\n",
        "1. EVALUATE MODEL PERFORMANCE\n",
        "   - Use Accuracy for overall performance\n",
        "   - Use Precision/Recall/F1 for detailed analysis\n",
        "   - Use Confusion Matrix to identify problem areas\n",
        "\n",
        "2. ANALYZE FEATURE IMPORTANCE\n",
        "   - Identify which features matter most\n",
        "   - Find redundant or useless features\n",
        "   - Understand model behavior\n",
        "\n",
        "3. IMPROVE MODEL\n",
        "   - Focus on classes with low F1-scores\n",
        "   - Improve or add features with high importance\n",
        "   - Remove features with zero importance\n",
        "   - Collect more data for confused classes\n",
        "\n",
        "4. ITERATE\n",
        "   - Re-evaluate after changes\n",
        "   - Compare metrics before/after\n",
        "   - Continue improving\n",
        "\n",
        "EVALUATION WORKFLOW:\n",
        "\n",
        "Step 1: Train Model\n",
        "  ‚Üì\n",
        "Step 2: Evaluate with Multiple Metrics\n",
        "  - Accuracy (overall)\n",
        "  - Precision/Recall/F1 (per-class)\n",
        "  - Confusion Matrix (errors)\n",
        "  ‚Üì\n",
        "Step 3: Analyze Feature Importance\n",
        "  - Which features matter?\n",
        "  - Which can be removed?\n",
        "  ‚Üì\n",
        "Step 4: Identify Problems\n",
        "  - Low F1-score classes\n",
        "  - Confused class pairs\n",
        "  - Useless features\n",
        "  ‚Üì\n",
        "Step 5: Make Improvements\n",
        "  - Feature engineering\n",
        "  - Data collection\n",
        "  - Hyperparameter tuning\n",
        "  ‚Üì\n",
        "Step 6: Re-evaluate\n",
        "  - Compare new metrics\n",
        "  - Check if problems fixed\n",
        "  ‚Üì\n",
        "Step 7: Repeat until satisfied\n",
        "\n",
        "BEST PRACTICES:\n",
        "\n",
        "‚úÖ DO:\n",
        "  - Use multiple metrics (not just accuracy)\n",
        "  - Check F1-score for each class\n",
        "  - Analyze confusion matrix for patterns\n",
        "  - Review feature importance regularly\n",
        "  - Consider business context when choosing metrics\n",
        "\n",
        "‚ùå DON'T:\n",
        "  - Rely only on accuracy\n",
        "  - Ignore low-importance features without checking\n",
        "  - Overlook confusion matrix patterns\n",
        "  - Optimize one metric at expense of others\n",
        "  - Forget to validate on test set\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUMMARY: KEY CONCEPTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "‚úÖ EVALUATION METRICS:\n",
        "   - Accuracy: Overall correctness\n",
        "   - Precision: Of predictions, how many correct?\n",
        "   - Recall: Of actual examples, how many found?\n",
        "   - F1-Score: Harmonic mean of Precision and Recall\n",
        "   - Support: Number of examples per class\n",
        "\n",
        "‚úÖ F1-SCORE:\n",
        "   - Balances Precision and Recall\n",
        "   - Better for imbalanced data than Accuracy\n",
        "   - Harmonic mean penalizes extreme values\n",
        "   - Variants: Micro, Macro, Weighted\n",
        "\n",
        "‚úÖ CONFUSION MATRIX:\n",
        "   - Visualizes prediction errors\n",
        "   - Shows which classes are confused\n",
        "   - Helps identify problem areas\n",
        "   - Guides model improvement\n",
        "\n",
        "‚úÖ FEATURE IMPORTANCE:\n",
        "   - Shows which features matter most\n",
        "   - Helps with feature selection\n",
        "   - Provides model interpretability\n",
        "   - Guides feature engineering\n",
        "\n",
        "‚úÖ BEST PRACTICES:\n",
        "   - Use multiple metrics\n",
        "   - Analyze per-class performance\n",
        "   - Review confusion matrix\n",
        "   - Check feature importance\n",
        "   - Iterate and improve\n",
        "\n",
        "NEXT STEPS:\n",
        "- Practice interpreting evaluation metrics\n",
        "- Experiment with different models\n",
        "- Analyze feature importance in your projects\n",
        "- Use metrics to guide model improvement\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EVALUATION & FEATURE IMPORTANCE GUIDE COMPLETE!\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'f1_score' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Compare to Week 1 model (assume old_rf from prior)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m old_f1 = \u001b[43mf1_score\u001b[49m(y_test, old_rf.predict(X_test), average=\u001b[33m'\u001b[39m\u001b[33mmacro\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m new_f1 = f1_score(y_test, y_pred, average=\u001b[33m'\u001b[39m\u001b[33mmacro\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m comparison = pd.DataFrame({\u001b[33m'\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33mOld RF\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTuned XGB\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m'\u001b[39m\u001b[33mF1\u001b[39m\u001b[33m'\u001b[39m: [old_f1, new_f1]})\n",
            "\u001b[31mNameError\u001b[39m: name 'f1_score' is not defined"
          ]
        }
      ],
      "source": [
        "# Compare to Week 1 model (assume old_rf from prior)\n",
        "old_f1 = f1_score(y_test, old_rf.predict(X_test), average='macro')\n",
        "new_f1 = f1_score(y_test, y_pred, average='macro')\n",
        "comparison = pd.DataFrame({'Model': ['Old RF', 'Tuned XGB'], 'F1': [old_f1, new_f1]})\n",
        "print(comparison)\n",
        "\n",
        "import joblib\n",
        "joblib.dump(best_xgb, 'src/enhanced_classifier.pkl')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
